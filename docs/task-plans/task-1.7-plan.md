# Task 1.7: í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ë¡œì§ êµ¬í˜„ - ì‹¤í–‰ ê³„íš

---

## ğŸ“‹ Meta

- **Task ID**: 1.7
- **Taskëª…**: í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í•  ë¡œì§ êµ¬í˜„
- **ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
- **ë‹´ë‹¹**: Backend
- **ì‘ì„±ì¼**: 2026-01-02
- **ìƒíƒœ**: Ready for Implementation
- **ë²„ì „**: 1.0.0

---

## 1. Executive Summary

### 1.1 ëª©í‘œ
íŒŒì‹±ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ ìµœì  í¬ê¸°ì˜ ì²­í¬(chunk)ë¡œ ë¶„í• í•˜ì—¬ ë²¡í„° ì„ë² ë”© ë° ê²€ìƒ‰ì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤. LangChainì˜ RecursiveCharacterTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ ë‹¨ìœ„ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë¶„í• í•©ë‹ˆë‹¤.

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­
- **ê¸°ëŠ¥**: í…ìŠ¤íŠ¸ë¥¼ 500ì ë‹¨ìœ„ë¡œ ë¶„í• , 50ì overlap
- **í’ˆì§ˆ**: ì²­í¬ í¬ê¸° í‰ê·  500ì Â± 10%, ë©”íƒ€ë°ì´í„° ìœ ì§€
- **ì„±ëŠ¥**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ (10,000ì ì´ìƒ)
- **í…ŒìŠ¤íŠ¸**: 10ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ 100% í†µê³¼

### 1.3 ì„±ê³µ ê¸°ì¤€
- [ ] ì²­í¬ í¬ê¸° ê²€ì¦ (í‰ê·  500ì Â± 10%)
- [ ] Overlap ê²€ì¦ (50ì)
- [ ] ë©”íƒ€ë°ì´í„° ìœ ì§€ í™•ì¸ (document_id, chunk_index, document_title, page_number)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼ (10ê°œ ì¼€ì´ìŠ¤)
- [ ] ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì„±ê³µ (10,000ì+)

### 1.4 Why This Task Matters
**RAG ì‹œìŠ¤í…œì˜ í•µì‹¬**:
- **ê²€ìƒ‰ ì •í™•ë„**: ì ì ˆí•œ ì²­í¬ í¬ê¸°ê°€ ê²€ìƒ‰ í’ˆì§ˆ ê²°ì •
- **ì»¨í…ìŠ¤íŠ¸ ìœ ì§€**: Overlapìœ¼ë¡œ ë¬¸ë§¥ ì—°ì†ì„± ë³´ì¥
- **ì„ë² ë”© íš¨ìœ¨**: í† í° ì œí•œ(~125 í† í°) ì¤€ìˆ˜
- **í™•ì¥ì„±**: ëª¨ë“  ë¬¸ì„œ íƒ€ì…ì— ì¬ì‚¬ìš©

---

## 2. ì„ í–‰ ì¡°ê±´ ê²€ì¦

### 2.1 í™˜ê²½ ê²€ì¦
ì‹¤í–‰ ì „ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# Python ë²„ì „ í™•ì¸ (3.11+ í•„ìš”)
python --version

# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
which python  # venv ê²½ë¡œì—¬ì•¼ í•¨

# Task 1.6 ì™„ë£Œ í™•ì¸
ls -la app/services/document_parser/
```

### 2.2 ì˜ì¡´ì„± í™•ì¸
ë‹¤ìŒ Taskë“¤ì´ ì™„ë£Œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

- [x] **Task 1.5**: PDF íŒŒì„œ êµ¬í˜„ ì™„ë£Œ
- [x] **Task 1.6**: DOCX, TXT, Markdown íŒŒì„œ êµ¬í˜„ ì™„ë£Œ

---

## 3. ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ

### 3.1 í…ìŠ¤íŠ¸ ë¶„í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹„êµ

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ì¥ì  | ë‹¨ì  | ì„ íƒ ì—¬ë¶€ |
|-----------|------|------|----------|
| **LangChain RecursiveCharacterTextSplitter** | - ì˜ë¯¸ ë‹¨ìœ„ ë¶„í• <br>- Overlap ì§€ì›<br>- ë‹¤ì–‘í•œ êµ¬ë¶„ì<br>- ë©”íƒ€ë°ì´í„° ìœ ì§€ | - LangChain ì˜ì¡´ì„± | â­ **ì„ íƒ** |
| **tiktoken + ìˆ˜ë™ ë¶„í• ** | - ì •í™•í•œ í† í° ìˆ˜<br>- ì˜ì¡´ì„± ì ìŒ | - ë³µì¡í•œ êµ¬í˜„<br>- ì˜ë¯¸ ë‹¨ìœ„ ë¬´ì‹œ | ë³´ë¥˜ |
| **spaCy + Sentence Split** | - ì •í™•í•œ ë¬¸ì¥ ë¶„í•  | - ë¬´ê±°ì›€<br>- Overlap êµ¬í˜„ ë³µì¡ | ë³´ë¥˜ |

### 3.2 ìµœì¢… ì„ íƒ: **LangChain RecursiveCharacterTextSplitter**

**ì„ íƒ ì´ìœ **:
1. **ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´**: ë¬¸ì¥, ë‹¨ë½ ê²½ê³„ ê³ ë ¤
2. **ê²€ì¦ëœ ì†”ë£¨ì…˜**: RAG ì‹œìŠ¤í…œ í‘œì¤€
3. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ êµ¬ë¶„ì ì»¤ìŠ¤í„°ë§ˆì´ì§•
4. **ë©”íƒ€ë°ì´í„° ì§€ì›**: ì²­í¬ë³„ ë©”íƒ€ë°ì´í„° ìë™ ê´€ë¦¬

**RecursiveCharacterTextSplitter ë™ì‘ ì›ë¦¬**:
```
1ì°¨ ì‹œë„: ë‹¨ë½ êµ¬ë¶„ì ("\n\n")ë¡œ ë¶„í• 
2ì°¨ ì‹œë„: ë¬¸ì¥ êµ¬ë¶„ì ("\n", ". ")ë¡œ ë¶„í• 
3ì°¨ ì‹œë„: ë‹¨ì–´ êµ¬ë¶„ì (" ")ë¡œ ë¶„í• 
4ì°¨ ì‹œë„: ë¬¸ì ë‹¨ìœ„ë¡œ ë¶„í•  (ìµœí›„ ìˆ˜ë‹¨)
```

**Overlapì˜ ì¤‘ìš”ì„±**:
- **ë¬¸ë§¥ ë³´ì¡´**: ì²­í¬ ê²½ê³„ì—ì„œ ì •ë³´ ì†ì‹¤ ë°©ì§€
- **ê²€ìƒ‰ í’ˆì§ˆ**: ê´€ë ¨ ì •ë³´ê°€ ì—¬ëŸ¬ ì²­í¬ì— ë¶„ì‚°ë˜ì–´ë„ ê²€ìƒ‰ ê°€ëŠ¥
- **ì˜ˆì‹œ**:
  ```
  Chunk 1: "...íšŒì‚¬ì˜ íœ´ê°€ ì •ì±…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì—°ì°¨ëŠ”..."
  Chunk 2: "ì—°ì°¨ëŠ” ì…ì‚¬ í›„ 1ë…„ë¶€í„° 15ì¼ ë¶€ì—¬ë©ë‹ˆë‹¤..."
  (Overlap: "ì—°ì°¨ëŠ”")
  ```

### 3.3 ì²­í¬ í¬ê¸° ì„¤ì • ê·¼ê±°

**chunk_size = 500ì**:
- **í† í° ë³€í™˜**: í•œê¸€/ì˜ì–´ í˜¼í•© ì‹œ ì•½ 125 í† í°
- **ì„ë² ë”© ëª¨ë¸**: nomic-embed-text ìµœëŒ€ í† í° ~512
- **ê²€ìƒ‰ ì •í™•ë„**: ë„ˆë¬´ í¬ë©´ ë…¸ì´ì¦ˆ, ë„ˆë¬´ ì‘ìœ¼ë©´ ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡±
- **LLM Context Window**: ì¶©ë¶„í•œ ì—¬ìœ  (5ê°œ ì²­í¬ = 625 í† í°)

**chunk_overlap = 50ì**:
- **Overlap ë¹„ìœ¨**: 10% (50/500)
- **ë¬¸ë§¥ ë³´ì¡´**: ë¬¸ì¥ 1-2ê°œ ì¤‘ë³µ
- **ì €ì¥ ê³µê°„**: ì¦ê°€ ìµœì†Œí™”

---

## 4. êµ¬í˜„ ë‹¨ê³„ë³„ ìƒì„¸ ê³„íš

### 4.1 Step 1: í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜ (15ë¶„)

#### ì‘ì—… ë‚´ìš©
1. **requirements.txt ì—…ë°ì´íŠ¸**
   ```txt
   langchain==0.1.0
   langchain-text-splitters==0.0.1
   ```

2. **ì˜ì¡´ì„± ì„¤ì¹˜**
   ```bash
   source venv/bin/activate
   pip install langchain==0.1.0 langchain-text-splitters==0.0.1
   pip freeze > requirements.txt
   ```

3. **íŒŒì¼ ìƒì„±**
   ```bash
   touch app/services/text_chunker.py
   touch tests/test_text_chunker.py
   ```

#### ê²€ì¦
```bash
# LangChain ì„¤ì¹˜ í™•ì¸
python -c "from langchain.text_splitter import RecursiveCharacterTextSplitter; print('OK')"
```

---

### 4.2 Step 2: í…ìŠ¤íŠ¸ ì²­ì»¤ í•µì‹¬ êµ¬í˜„ (60ë¶„)

#### ì‘ì—… ë‚´ìš©
`app/services/text_chunker.py` ì‘ì„±

**ì„¤ê³„ ì›ì¹™**:
- **ë‹¨ì¼ ì±…ì„**: í…ìŠ¤íŠ¸ ë¶„í• ë§Œ ë‹´ë‹¹
- **ë¶ˆë³€ì„±**: ì›ë³¸ ë¬¸ì„œ ìˆ˜ì • ì•ˆ í•¨
- **íƒ€ì… ì•ˆì „ì„±**: Pydantic ëª¨ë¸ ì‚¬ìš©

```python
import logging
from typing import List, Dict, Any, Optional
from pydantic import BaseModel, Field
from langchain.text_splitter import RecursiveCharacterTextSplitter

from app.services.document_parser.base_parser import ParsedDocument, ParsedPage

logger = logging.getLogger(__name__)


class TextChunk(BaseModel):
    """í…ìŠ¤íŠ¸ ì²­í¬ ë°ì´í„° ëª¨ë¸"""

    chunk_index: int = Field(..., ge=0, description="ì²­í¬ ì¸ë±ìŠ¤ (0ë¶€í„° ì‹œì‘)")
    content: str = Field(..., min_length=1, description="ì²­í¬ í…ìŠ¤íŠ¸ ë‚´ìš©")
    start_char: int = Field(..., ge=0, description="ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì‹œì‘ ìœ„ì¹˜")
    end_char: int = Field(..., ge=0, description="ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ì¢…ë£Œ ìœ„ì¹˜")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="ì²­í¬ ë©”íƒ€ë°ì´í„°")

    class Config:
        frozen = False  # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ í—ˆìš©


class ChunkerConfig(BaseModel):
    """í…ìŠ¤íŠ¸ ì²­ì»¤ ì„¤ì •"""

    chunk_size: int = Field(default=500, ge=100, le=2000, description="ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)")
    chunk_overlap: int = Field(default=50, ge=0, le=500, description="ì²­í¬ Overlap (ë¬¸ì ìˆ˜)")
    separators: List[str] = Field(
        default=["\n\n", "\n", ". ", " ", ""],
        description="ë¶„í•  êµ¬ë¶„ì (ìš°ì„ ìˆœìœ„ ìˆœì„œ)"
    )
    keep_separator: bool = Field(default=True, description="êµ¬ë¶„ì ìœ ì§€ ì—¬ë¶€")


class DocumentChunker:
    """ë¬¸ì„œ í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• ê¸°"""

    def __init__(self, config: Optional[ChunkerConfig] = None):
        """
        Args:
            config: ì²­ì»¤ ì„¤ì • (ê¸°ë³¸ê°’ ì‚¬ìš© ê°€ëŠ¥)
        """
        self.config = config or ChunkerConfig()

        # LangChain RecursiveCharacterTextSplitter ì´ˆê¸°í™”
        self.splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.chunk_size,
            chunk_overlap=self.config.chunk_overlap,
            separators=self.config.separators,
            keep_separator=self.config.keep_separator,
            length_function=len,  # ë¬¸ì ìˆ˜ ê¸°ì¤€
        )

        logger.info(
            f"DocumentChunker ì´ˆê¸°í™”: chunk_size={self.config.chunk_size}, "
            f"overlap={self.config.chunk_overlap}"
        )

    def chunk_document(self, parsed_doc: ParsedDocument) -> List[TextChunk]:
        """
        íŒŒì‹±ëœ ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• 

        Args:
            parsed_doc: ParsedDocument (Task 1.5/1.6ì—ì„œ ìƒì„±)

        Returns:
            List[TextChunk]: ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸

        Raises:
            ValueError: ë¹ˆ ë¬¸ì„œì¸ ê²½ìš°
        """
        logger.info(
            f"ë¬¸ì„œ ì²­í‚¹ ì‹œì‘: {parsed_doc.total_pages}í˜ì´ì§€, "
            f"{parsed_doc.total_characters}ì"
        )

        if parsed_doc.total_characters == 0:
            logger.warning("ë¹ˆ ë¬¸ì„œì…ë‹ˆë‹¤. ì²­í¬ ìƒì„± ë¶ˆê°€.")
            return []

        # Step 1: ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ (í˜ì´ì§€ë³„ êµ¬ë¶„ ìœ ì§€)
        full_text, page_boundaries = self._extract_full_text(parsed_doc)

        # Step 2: í…ìŠ¤íŠ¸ ë¶„í• 
        text_chunks = self.splitter.split_text(full_text)

        # Step 3: TextChunk ê°ì²´ ìƒì„± (ë©”íƒ€ë°ì´í„° í¬í•¨)
        chunks = []
        current_pos = 0

        for idx, chunk_text in enumerate(text_chunks):
            # ì²­í¬ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ì°¾ê¸°
            start_pos = full_text.find(chunk_text, current_pos)
            if start_pos == -1:
                # Overlapìœ¼ë¡œ ì¸í•´ ì¤‘ë³µëœ ê²½ìš°, ì´ì „ ìœ„ì¹˜ë¶€í„° ì¬ê²€ìƒ‰
                start_pos = current_pos
            end_pos = start_pos + len(chunk_text)

            # ì²­í¬ê°€ ì†í•œ í˜ì´ì§€ ë²ˆí˜¸ ê²°ì •
            page_number = self._find_page_number(start_pos, page_boundaries)

            # TextChunk ìƒì„±
            chunk = TextChunk(
                chunk_index=idx,
                content=chunk_text,
                start_char=start_pos,
                end_char=end_pos,
                metadata={
                    "page_number": page_number,
                    "document_title": parsed_doc.metadata.get("title", "Untitled"),
                    "total_chunks": len(text_chunks),  # ì„ì‹œ, ë‚˜ì¤‘ì— ì—…ë°ì´íŠ¸
                    "chunk_length": len(chunk_text),
                }
            )

            chunks.append(chunk)
            current_pos = start_pos + 1  # ë‹¤ìŒ ê²€ìƒ‰ ìœ„ì¹˜

        # Step 4: total_chunks ì—…ë°ì´íŠ¸
        total_chunks = len(chunks)
        for chunk in chunks:
            chunk.metadata["total_chunks"] = total_chunks

        logger.info(f"ë¬¸ì„œ ì²­í‚¹ ì™„ë£Œ: {total_chunks}ê°œ ì²­í¬ ìƒì„±")

        return chunks

    def chunk_text(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> List[TextChunk]:
        """
        ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì²­í‚¹ (ParsedDocument ì—†ì´)

        Args:
            text: ë¶„í• í•  í…ìŠ¤íŠ¸
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒ)

        Returns:
            List[TextChunk]: ë¶„í• ëœ ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        if not text.strip():
            return []

        text_chunks = self.splitter.split_text(text)

        chunks = []
        current_pos = 0

        for idx, chunk_text in enumerate(text_chunks):
            start_pos = text.find(chunk_text, current_pos)
            if start_pos == -1:
                start_pos = current_pos
            end_pos = start_pos + len(chunk_text)

            chunk_metadata = metadata.copy() if metadata else {}
            chunk_metadata.update({
                "total_chunks": len(text_chunks),
                "chunk_length": len(chunk_text),
            })

            chunk = TextChunk(
                chunk_index=idx,
                content=chunk_text,
                start_char=start_pos,
                end_char=end_pos,
                metadata=chunk_metadata,
            )

            chunks.append(chunk)
            current_pos = start_pos + 1

        return chunks

    def _extract_full_text(
        self, parsed_doc: ParsedDocument
    ) -> tuple[str, List[tuple[int, int]]]:
        """
        ParsedDocumentì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° í˜ì´ì§€ ê²½ê³„ ê³„ì‚°

        Args:
            parsed_doc: ParsedDocument

        Returns:
            (full_text, page_boundaries)
            - full_text: ì „ì²´ í…ìŠ¤íŠ¸ (í˜ì´ì§€ êµ¬ë¶„ì í¬í•¨)
            - page_boundaries: [(start_pos, page_number), ...]
        """
        full_text = ""
        page_boundaries = []
        current_pos = 0

        for page in parsed_doc.pages:
            page_text = page.content

            # í˜ì´ì§€ ê²½ê³„ ê¸°ë¡
            page_boundaries.append((current_pos, page.page_number))

            # í…ìŠ¤íŠ¸ ì¶”ê°€ (í˜ì´ì§€ êµ¬ë¶„ìë¡œ "\n\n" ì‚¬ìš©)
            full_text += page_text
            if page.page_number < parsed_doc.total_pages:
                full_text += "\n\n"  # í˜ì´ì§€ êµ¬ë¶„ì

            current_pos = len(full_text)

        return full_text, page_boundaries

    def _find_page_number(
        self, char_position: int, page_boundaries: List[tuple[int, int]]
    ) -> int:
        """
        ë¬¸ì ìœ„ì¹˜ë¡œë¶€í„° í˜ì´ì§€ ë²ˆí˜¸ ì°¾ê¸°

        Args:
            char_position: ë¬¸ì ìœ„ì¹˜
            page_boundaries: í˜ì´ì§€ ê²½ê³„ ë¦¬ìŠ¤íŠ¸

        Returns:
            í˜ì´ì§€ ë²ˆí˜¸
        """
        for i in range(len(page_boundaries) - 1, -1, -1):
            boundary_pos, page_num = page_boundaries[i]
            if char_position >= boundary_pos:
                return page_num

        # ê¸°ë³¸ê°’: ì²« í˜ì´ì§€
        return page_boundaries[0][1] if page_boundaries else 1

    def get_chunk_statistics(self, chunks: List[TextChunk]) -> Dict[str, Any]:
        """
        ì²­í¬ í†µê³„ ê³„ì‚°

        Args:
            chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if not chunks:
            return {
                "total_chunks": 0,
                "avg_chunk_size": 0,
                "min_chunk_size": 0,
                "max_chunk_size": 0,
                "total_characters": 0,
            }

        chunk_sizes = [len(chunk.content) for chunk in chunks]

        return {
            "total_chunks": len(chunks),
            "avg_chunk_size": sum(chunk_sizes) / len(chunk_sizes),
            "min_chunk_size": min(chunk_sizes),
            "max_chunk_size": max(chunk_sizes),
            "total_characters": sum(chunk_sizes),
            "std_deviation": self._calculate_std(chunk_sizes),
        }

    @staticmethod
    def _calculate_std(values: List[float]) -> float:
        """í‘œì¤€ í¸ì°¨ ê³„ì‚°"""
        if len(values) < 2:
            return 0.0

        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / len(values)
        return variance ** 0.5
```

---

### 4.3 Step 3: TDD - í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‘ì„± (60ë¶„)

#### ì‘ì—… ë‚´ìš©
`tests/test_text_chunker.py` ì‘ì„± (10ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤)

```python
import pytest
from app.services.text_chunker import DocumentChunker, ChunkerConfig, TextChunk
from app.services.document_parser.base_parser import ParsedDocument, ParsedPage


@pytest.fixture
def default_chunker():
    """ê¸°ë³¸ ì„¤ì • ì²­ì»¤"""
    return DocumentChunker()


@pytest.fixture
def custom_chunker():
    """ì»¤ìŠ¤í…€ ì„¤ì • ì²­ì»¤"""
    config = ChunkerConfig(chunk_size=300, chunk_overlap=30)
    return DocumentChunker(config=config)


@pytest.fixture
def sample_parsed_doc():
    """ìƒ˜í”Œ ParsedDocument"""
    page1 = ParsedPage(
        page_number=1,
        content="ì´ê²ƒì€ ì²« ë²ˆì§¸ í˜ì´ì§€ì…ë‹ˆë‹¤. " * 50,  # ì•½ 600ì
        metadata={}
    )
    page2 = ParsedPage(
        page_number=2,
        content="ì´ê²ƒì€ ë‘ ë²ˆì§¸ í˜ì´ì§€ì…ë‹ˆë‹¤. " * 50,  # ì•½ 600ì
        metadata={}
    )
    return ParsedDocument(
        pages=[page1, page2],
        total_pages=2,
        total_characters=1200,
        metadata={"title": "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ"}
    )


# ============================================
# Happy Path Tests (ì •ìƒ ì‹œë‚˜ë¦¬ì˜¤)
# ============================================

def test_basic_chunking(default_chunker):
    """TC01: ê¸°ë³¸ ì²­í‚¹ ë™ì‘"""
    text = "This is a test. " * 100  # ì•½ 1600ì

    chunks = default_chunker.chunk_text(text)

    assert len(chunks) > 0
    assert all(isinstance(chunk, TextChunk) for chunk in chunks)
    assert chunks[0].chunk_index == 0


def test_chunk_size_constraint(default_chunker):
    """TC02: ì²­í¬ í¬ê¸° ì œì•½ (í‰ê·  500ì Â± 10%)"""
    text = "Lorem ipsum dolor sit amet. " * 200  # ì•½ 5600ì

    chunks = default_chunker.chunk_text(text)
    stats = default_chunker.get_chunk_statistics(chunks)

    avg_size = stats["avg_chunk_size"]
    assert 450 <= avg_size <= 550, f"í‰ê·  ì²­í¬ í¬ê¸°: {avg_size} (ëª©í‘œ: 450-550)"


def test_chunk_overlap(default_chunker):
    """TC03: Overlap ê²€ì¦ (50ì)"""
    # ëª…í™•í•œ êµ¬ë¶„ì´ ìˆëŠ” í…ìŠ¤íŠ¸
    text = "Section A. " * 50 + "Section B. " * 50  # ì•½ 1100ì

    chunks = default_chunker.chunk_text(text)

    # ì¸ì ‘ ì²­í¬ ê°„ ì¤‘ë³µ í™•ì¸
    if len(chunks) > 1:
        for i in range(len(chunks) - 1):
            chunk1_end = chunks[i].content[-50:]  # ë§ˆì§€ë§‰ 50ì
            chunk2_start = chunks[i + 1].content[:50]  # ì²˜ìŒ 50ì

            # ì¼ë¶€ ì¤‘ë³µì´ ìˆì–´ì•¼ í•¨
            overlap_found = any(
                word in chunk2_start for word in chunk1_end.split()
            )
            assert overlap_found, f"ì²­í¬ {i}ì™€ {i+1} ì‚¬ì´ì— overlap ì—†ìŒ"


def test_metadata_preservation(default_chunker, sample_parsed_doc):
    """TC04: ë©”íƒ€ë°ì´í„° ìœ ì§€ í™•ì¸"""
    chunks = default_chunker.chunk_document(sample_parsed_doc)

    for chunk in chunks:
        assert "page_number" in chunk.metadata
        assert "document_title" in chunk.metadata
        assert "total_chunks" in chunk.metadata
        assert "chunk_length" in chunk.metadata

        # document_title í™•ì¸
        assert chunk.metadata["document_title"] == "í…ŒìŠ¤íŠ¸ ë¬¸ì„œ"


def test_page_number_tracking(default_chunker):
    """TC05: í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì """
    page1 = ParsedPage(page_number=1, content="A" * 600, metadata={})
    page2 = ParsedPage(page_number=2, content="B" * 600, metadata={})
    page3 = ParsedPage(page_number=3, content="C" * 600, metadata={})

    doc = ParsedDocument(
        pages=[page1, page2, page3],
        total_pages=3,
        total_characters=1800,
        metadata={}
    )

    chunks = default_chunker.chunk_document(doc)

    # ê° ì²­í¬ì˜ í˜ì´ì§€ ë²ˆí˜¸ê°€ 1, 2, 3 ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•¨
    page_numbers = [chunk.metadata["page_number"] for chunk in chunks]
    assert all(1 <= pn <= 3 for pn in page_numbers)


# ============================================
# Edge Cases (ê²½ê³„ ì¡°ê±´)
# ============================================

def test_empty_document(default_chunker):
    """TC06: ë¹ˆ ë¬¸ì„œ ì²˜ë¦¬"""
    doc = ParsedDocument(
        pages=[],
        total_pages=0,
        total_characters=0,
        metadata={}
    )

    chunks = default_chunker.chunk_document(doc)

    assert chunks == []


def test_short_text(default_chunker):
    """TC07: ì²­í¬ í¬ê¸°ë³´ë‹¤ ì§§ì€ í…ìŠ¤íŠ¸"""
    text = "Short text."  # 11ì

    chunks = default_chunker.chunk_text(text)

    assert len(chunks) == 1
    assert chunks[0].content == text


def test_exact_chunk_size(default_chunker):
    """TC08: ì •í™•íˆ ì²­í¬ í¬ê¸°ì¸ í…ìŠ¤íŠ¸"""
    text = "A" * 500  # ì •í™•íˆ 500ì

    chunks = default_chunker.chunk_text(text)

    assert len(chunks) == 1
    assert len(chunks[0].content) == 500


def test_very_long_document(default_chunker):
    """TC09: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ (10,000ì+)"""
    text = "Lorem ipsum dolor sit amet, consectetur adipiscing elit. " * 200
    # ì•½ 11,400ì

    chunks = default_chunker.chunk_text(text)

    assert len(chunks) > 10  # ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
    stats = default_chunker.get_chunk_statistics(chunks)
    assert stats["total_characters"] >= 10000


# ============================================
# Configuration Tests (ì„¤ì • í…ŒìŠ¤íŠ¸)
# ============================================

def test_custom_chunk_size(custom_chunker):
    """TC10: ì»¤ìŠ¤í…€ ì²­í¬ í¬ê¸° (300ì)"""
    text = "Test text. " * 100  # ì•½ 1100ì

    chunks = custom_chunker.chunk_text(text)
    stats = custom_chunker.get_chunk_statistics(chunks)

    avg_size = stats["avg_chunk_size"]
    # 300ì Â± 10%
    assert 270 <= avg_size <= 330, f"í‰ê·  ì²­í¬ í¬ê¸°: {avg_size} (ëª©í‘œ: 270-330)"


# ============================================
# Statistics Tests (í†µê³„ í…ŒìŠ¤íŠ¸)
# ============================================

def test_chunk_statistics(default_chunker):
    """TC11: ì²­í¬ í†µê³„ ê³„ì‚°"""
    text = "Sample text. " * 150

    chunks = default_chunker.chunk_text(text)
    stats = default_chunker.get_chunk_statistics(chunks)

    assert "total_chunks" in stats
    assert "avg_chunk_size" in stats
    assert "min_chunk_size" in stats
    assert "max_chunk_size" in stats
    assert "total_characters" in stats
    assert "std_deviation" in stats

    assert stats["total_chunks"] == len(chunks)
    assert stats["avg_chunk_size"] > 0
```

---

### 4.4 Step 4: í†µí•© í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ (45ë¶„)

#### ì‘ì—… ë‚´ìš©

1. **ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰**
   ```bash
   # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
   pytest tests/test_text_chunker.py -v

   # ì»¤ë²„ë¦¬ì§€ í™•ì¸ (ëª©í‘œ: 95% ì´ìƒ)
   pytest tests/test_text_chunker.py --cov=app/services/text_chunker --cov-report=term-missing
   ```

2. **ì‹¤ì œ ë¬¸ì„œ í†µí•© í…ŒìŠ¤íŠ¸**
   `tests/test_chunker_integration.py`:

   ```python
   import pytest
   from pathlib import Path
   from app.services.document_parser.factory import DocumentParserFactory
   from app.services.text_chunker import DocumentChunker


   def test_pdf_to_chunks_integration():
       """PDF â†’ íŒŒì‹± â†’ ì²­í‚¹ í†µí•© í…ŒìŠ¤íŠ¸"""
       pdf_path = "tests/fixtures/pdf/sample_valid.pdf"

       if not Path(pdf_path).exists():
           pytest.skip("PDF íŒŒì¼ ì—†ìŒ")

       # Step 1: PDF íŒŒì‹±
       parser = DocumentParserFactory.create_parser(pdf_path)
       parsed_doc = parser.parse(pdf_path)

       # Step 2: ì²­í‚¹
       chunker = DocumentChunker()
       chunks = chunker.chunk_document(parsed_doc)

       # Step 3: ê²€ì¦
       assert len(chunks) > 0
       assert all(chunk.metadata["document_title"] for chunk in chunks)

       # í†µê³„ ì¶œë ¥
       stats = chunker.get_chunk_statistics(chunks)
       print(f"\nğŸ“Š ì²­í¬ í†µê³„:")
       print(f"  - ì´ ì²­í¬ ìˆ˜: {stats['total_chunks']}")
       print(f"  - í‰ê·  í¬ê¸°: {stats['avg_chunk_size']:.1f}ì")
       print(f"  - ìµœì†Œ í¬ê¸°: {stats['min_chunk_size']}ì")
       print(f"  - ìµœëŒ€ í¬ê¸°: {stats['max_chunk_size']}ì")


   def test_all_document_types_chunking():
       """ëª¨ë“  ë¬¸ì„œ íƒ€ì… ì²­í‚¹ í…ŒìŠ¤íŠ¸"""
       test_files = [
           "tests/fixtures/pdf/sample_valid.pdf",
           "tests/fixtures/docx/sample_valid.docx",
           "tests/fixtures/txt/sample_valid.txt",
           "tests/fixtures/markdown/sample_valid.md",
       ]

       chunker = DocumentChunker()

       for file_path in test_files:
           if not Path(file_path).exists():
               continue

           # íŒŒì‹±
           parser = DocumentParserFactory.create_parser(file_path)
           parsed_doc = parser.parse(file_path)

           # ì²­í‚¹
           chunks = chunker.chunk_document(parsed_doc)

           assert len(chunks) > 0
           print(f"âœ“ {Path(file_path).name}: {len(chunks)} chunks")
   ```

3. **ì„±ëŠ¥ í…ŒìŠ¤íŠ¸**
   ```python
   def test_large_document_performance():
       """ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
       import time

       # 10,000ì ë¬¸ì„œ ìƒì„±
       text = "Lorem ipsum dolor sit amet. " * 400

       chunker = DocumentChunker()

       start_time = time.time()
       chunks = chunker.chunk_text(text)
       elapsed = time.time() - start_time

       assert elapsed < 1.0, f"ì²­í‚¹ ì‹œê°„: {elapsed:.2f}ì´ˆ (ëª©í‘œ: < 1ì´ˆ)"
       print(f"\nâ±ï¸  ì²­í‚¹ ì‹œê°„: {elapsed:.3f}ì´ˆ ({len(chunks)} chunks)")
   ```

---

### 4.5 Step 5: ë¬¸ì„œí™” ë° ì˜ˆì œ ì‘ì„± (30min)

#### ì‘ì—… ë‚´ìš©

**README ì—…ë°ì´íŠ¸** (`docs/api/text_chunker_usage.md`):

```markdown
# Text Chunker Usage Guide

## ê°œìš”
`DocumentChunker`ëŠ” íŒŒì‹±ëœ ë¬¸ì„œë¥¼ ìµœì  í¬ê¸°ì˜ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤.

## ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ParsedDocument ì²­í‚¹
```python
from app.services.document_parser.pdf_parser import PDFParser
from app.services.text_chunker import DocumentChunker

# PDF íŒŒì‹±
parser = PDFParser()
parsed_doc = parser.parse("document.pdf")

# ì²­í‚¹
chunker = DocumentChunker()
chunks = chunker.chunk_document(parsed_doc)

# ê²°ê³¼ í™•ì¸
for chunk in chunks:
    print(f"Chunk {chunk.chunk_index}: {len(chunk.content)}ì")
    print(f"  í˜ì´ì§€: {chunk.metadata['page_number']}")
    print(f"  ë‚´ìš©: {chunk.content[:100]}...")
```

### 2. ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì²­í‚¹
```python
from app.services.text_chunker import DocumentChunker

text = "ê¸´ í…ìŠ¤íŠ¸..."
chunker = DocumentChunker()
chunks = chunker.chunk_text(text, metadata={"source": "manual"})
```

### 3. ì»¤ìŠ¤í…€ ì„¤ì •
```python
from app.services.text_chunker import DocumentChunker, ChunkerConfig

config = ChunkerConfig(
    chunk_size=300,
    chunk_overlap=30,
    separators=["\n\n", "\n", " "]
)
chunker = DocumentChunker(config=config)
```

## ì²­í¬ í†µê³„
```python
stats = chunker.get_chunk_statistics(chunks)
print(f"í‰ê·  í¬ê¸°: {stats['avg_chunk_size']:.1f}ì")
print(f"í‘œì¤€ í¸ì°¨: {stats['std_deviation']:.1f}ì")
```

## ë©”íƒ€ë°ì´í„° êµ¬ì¡°
ê° ì²­í¬ëŠ” ë‹¤ìŒ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤:
- `page_number`: ì›ë³¸ í˜ì´ì§€ ë²ˆí˜¸
- `document_title`: ë¬¸ì„œ ì œëª©
- `total_chunks`: ì „ì²´ ì²­í¬ ìˆ˜
- `chunk_length`: ì²­í¬ ê¸¸ì´
```

---

## 5. ê²€ì¦ ë° ìˆ˜ë™ í…ŒìŠ¤íŠ¸

### 5.1 ìë™í™” í…ŒìŠ¤íŠ¸ ê²€ì¦
```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/test_text_chunker.py -v

# ì˜ˆìƒ ê²°ê³¼:
# tests/test_text_chunker.py::test_basic_chunking PASSED              [  9%]
# tests/test_text_chunker.py::test_chunk_size_constraint PASSED      [ 18%]
# tests/test_text_chunker.py::test_chunk_overlap PASSED              [ 27%]
# tests/test_text_chunker.py::test_metadata_preservation PASSED      [ 36%]
# tests/test_text_chunker.py::test_page_number_tracking PASSED       [ 45%]
# tests/test_text_chunker.py::test_empty_document PASSED             [ 54%]
# tests/test_text_chunker.py::test_short_text PASSED                 [ 63%]
# tests/test_text_chunker.py::test_exact_chunk_size PASSED           [ 72%]
# tests/test_text_chunker.py::test_very_long_document PASSED         [ 81%]
# tests/test_text_chunker.py::test_custom_chunk_size PASSED          [ 90%]
# tests/test_text_chunker.py::test_chunk_statistics PASSED           [100%]
#
# ========================== 11 passed in 1.24s ==========================
```

### 5.2 ìˆ˜ë™ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] ì²­í¬ í¬ê¸° í‰ê·  500ì Â± 10% í™•ì¸
- [ ] ì¸ì ‘ ì²­í¬ ê°„ overlap í™•ì¸ (ìœ¡ì•ˆ)
- [ ] í˜ì´ì§€ ë²ˆí˜¸ ì •í™•ì„± í™•ì¸
- [ ] ë©”íƒ€ë°ì´í„° ì™„ì „ì„± í™•ì¸
- [ ] ëŒ€ìš©ëŸ‰ ë¬¸ì„œ (10,000ì+) ì²˜ë¦¬ ì„±ê³µ
- [ ] ì„±ëŠ¥: 10,000ì ë¬¸ì„œ ì²­í‚¹ < 1ì´ˆ

---

## 6. ì‚°ì¶œë¬¼ (Deliverables)

### 6.1 ì½”ë“œ íŒŒì¼
- [x] `app/services/text_chunker.py` (í•µì‹¬ ë¡œì§)
- [x] `tests/test_text_chunker.py` (11ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤)
- [x] `tests/test_chunker_integration.py` (í†µí•© í…ŒìŠ¤íŠ¸)

### 6.2 ë¬¸ì„œ
- [x] `docs/api/text_chunker_usage.md` (ì‚¬ìš© ê°€ì´ë“œ)

---

## 7. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘ ë°©ì•ˆ

### 7.1 ê¸°ìˆ  ë¦¬ìŠ¤í¬

#### Risk 1: ì²­í¬ í¬ê¸° ë¶ˆê· í˜•
**ì¦ìƒ**:
- ì¼ë¶€ ì²­í¬ê°€ ë§¤ìš° í¬ê±°ë‚˜ ì‘ìŒ
- í‰ê·  500ì Â± 10% ë²—ì–´ë‚¨

**í™•ë¥ **: Medium (30%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - RecursiveCharacterTextSplitterì˜ separator ì¡°ì •
   - í•œê¸€ ë¬¸ì„œ: `["\n\n", "\n", ".", " "]` ìˆœì„œ ìµœì í™”

2. **ë‹¨ê¸° ëŒ€ì‘**:
   - í›„ì²˜ë¦¬ ë¡œì§ ì¶”ê°€ (ë„ˆë¬´ ì‘ì€ ì²­í¬ ë³‘í•©)
   - ì²­í¬ í¬ê¸° ë™ì  ì¡°ì •

---

#### Risk 2: ì˜ë¯¸ ë‹¨ìœ„ ë¶„ë¦¬ (ë¬¸ì¥ ì¤‘ê°„ ë¶„í• )
**ì¦ìƒ**:
- ì²­í¬ê°€ ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ëŠê¹€
- ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤

**í™•ë¥ **: Low (15%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - `keep_separator=True` ìœ ì§€ (êµ¬ë¶„ì ë³´ì¡´)
   - Overlap 50ìë¡œ ë¬¸ë§¥ ë³´ì¡´

2. **ë‹¨ê¸° ëŒ€ì‘**:
   - spaCy ë¬¸ì¥ ë¶„í• ê¸° ì¶”ê°€ ê³ ë ¤
   - ì²­í¬ ê²½ê³„ë¥¼ ë¬¸ì¥ ê²½ê³„ë¡œ ì¡°ì •

---

#### Risk 3: í•œê¸€ í† í° ë³€í™˜ ë¶€ì •í™•
**ì¦ìƒ**:
- 500ì â‰  125 í† í° (ì˜ˆìƒê³¼ ë‹¤ë¦„)
- ì„ë² ë”© ëª¨ë¸ í† í° ì œí•œ ì´ˆê³¼

**í™•ë¥ **: Medium (25%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - 500ì ê¸°ì¤€ ìœ ì§€ (ì•ˆì „ ë§ˆì§„ í™•ë³´)
   - ì‹¤ì œ í† í° ìˆ˜ ì¸¡ì • í…ŒìŠ¤íŠ¸

2. **ë‹¨ê¸° ëŒ€ì‘** (Task 1.8):
   - tiktoken ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‹¤ì œ í† í° ìˆ˜ ê³„ì‚°
   - í•„ìš” ì‹œ chunk_size ì¡°ì • (500 â†’ 450)

---

#### Risk 4: LangChain ì˜ì¡´ì„± ë²„ì „ ì¶©ëŒ
**ì¦ìƒ**:
- RecursiveCharacterTextSplitter API ë³€ê²½
- ì„¤ì¹˜ ì˜¤ë¥˜

**í™•ë¥ **: Low (10%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - ë²„ì „ ê³ ì •: `langchain==0.1.0`
   - requirements.txt ëª…ì‹œ

2. **ì¥ê¸° ëŒ€ì‘**:
   - LangChain ì—†ì´ ìì²´ êµ¬í˜„ ê³ ë ¤

---

## 8. Next Steps (Task 1.7 ì™„ë£Œ í›„)

### 8.1 ì¦‰ì‹œ ìˆ˜í–‰
1. **ì½”ë“œ ë¦¬ë·° ìš”ì²­**
   - Backend Leadì—ê²Œ ë¦¬ë·° ìš”ì²­
   - ì²­í¬ í¬ê¸° ì „ëµ ê²€í† 

2. **ë¬¸ì„œí™”**
   - API ë¬¸ì„œ ì‘ì„±
   - ì‚¬ìš© ì˜ˆì œ ì¶”ê°€

3. **Git ì»¤ë°‹**
   ```bash
   git add .
   git commit -m "feat: Implement text chunking logic (Task 1.7)

   - Add DocumentChunker with LangChain RecursiveCharacterTextSplitter
   - Implement chunk_size=500, chunk_overlap=50 configuration
   - Add metadata tracking (page_number, document_title, chunk_index)
   - Add 11 comprehensive test cases (100% passing)
   - Add integration tests with all document parsers
   - Add chunk statistics calculation
   - Add performance test (<1s for 10k chars)

   Closes #7"
   ```

### 8.2 ë‹¤ìŒ Task ì¤€ë¹„
**Task 1.8: ë¬¸ì„œ ì„ë² ë”© ë° Milvus ì €ì¥** ì¤€ë¹„:
- [ ] Ollama nomic-embed-text ëª¨ë¸ í™•ì¸
- [ ] Milvus Collection ìŠ¤í‚¤ë§ˆ ê²€í† 
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì „ëµ ì„¤ê³„

---

## 9. Appendix

### 9.1 ì°¸ê³  ìë£Œ
- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [RecursiveCharacterTextSplitter API](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html)
- [RAG Chunking Strategies](https://www.pinecone.io/learn/chunking-strategies/)

### 9.2 ì²­í¬ í¬ê¸° ì„¤ì • ê°€ì´ë“œ

| ë¬¸ì„œ íƒ€ì… | ê¶Œì¥ chunk_size | ê¶Œì¥ overlap | ì´ìœ  |
|----------|----------------|--------------|------|
| ê¸°ìˆ  ë¬¸ì„œ | 500-700 | 50-100 | ê¸´ ì„¤ëª…, ì˜ˆì œ ì½”ë“œ í¬í•¨ |
| ë‰´ìŠ¤ ê¸°ì‚¬ | 300-500 | 30-50 | ì§§ì€ ë¬¸ì¥, ë¹ ë¥¸ ê²€ìƒ‰ |
| ë²•ë¥  ë¬¸ì„œ | 700-1000 | 100-150 | ë³µì¡í•œ ë¬¸ë§¥, ì •í™•ë„ ì¤‘ìš” |
| ì±„íŒ… ë¡œê·¸ | 200-300 | 20-30 | ì§§ì€ ë©”ì‹œì§€, ë¹ ë¥¸ ì‘ë‹µ |

### 9.3 ìœ ìš©í•œ ì»¤ë§¨ë“œ
```bash
# íŠ¹ì • í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
pytest tests/test_text_chunker.py::test_chunk_size_constraint -v

# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/test_chunker_integration.py -v

# ì»¤ë²„ë¦¬ì§€ í™•ì¸
pytest tests/test_text_chunker.py --cov=app/services/text_chunker --cov-report=html

# ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
pytest tests/test_text_chunker.py::test_large_document_performance -v -s
```

### 9.4 íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**ë¬¸ì œ**: `ModuleNotFoundError: No module named 'langchain'`
**í•´ê²°**:
```bash
source venv/bin/activate
pip install langchain==0.1.0 langchain-text-splitters==0.0.1
```

**ë¬¸ì œ**: ì²­í¬ í¬ê¸°ê°€ ë„ˆë¬´ ë¶ˆê· ì¼í•¨
**í•´ê²°**:
```python
# separator ìˆœì„œ ì¡°ì •
config = ChunkerConfig(
    separators=["\n\n", "\n", ". ", "ã€‚", " ", ""]  # í•œê¸€ ë§ˆì¹¨í‘œ ì¶”ê°€
)
```

**ë¬¸ì œ**: Overlapì´ ì œëŒ€ë¡œ ë™ì‘í•˜ì§€ ì•ŠìŒ
**í•´ê²°**:
```python
# keep_separator í™•ì¸
config = ChunkerConfig(
    chunk_overlap=50,
    keep_separator=True  # êµ¬ë¶„ì ìœ ì§€ í•„ìˆ˜
)
```

---

## 10. Approval & Sign-off

### 10.1 ì²´í¬ë¦¬ìŠ¤íŠ¸
Task 1.7 ì™„ë£Œ ì¡°ê±´:
- [ ] ì²­í¬ í¬ê¸° í‰ê·  500ì Â± 10% ë‹¬ì„±
- [ ] Overlap 50ì ê²€ì¦ í†µê³¼
- [ ] ë©”íƒ€ë°ì´í„° ìœ ì§€ í™•ì¸ (4ê°œ í•„ë“œ)
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ 11ê°œ ëª¨ë‘ í†µê³¼
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼ (ëª¨ë“  ë¬¸ì„œ íƒ€ì…)
- [ ] ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ì²˜ë¦¬ ì„±ê³µ (10,000ì+)
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼ (< 1ì´ˆ)
- [ ] ì½”ë“œ ì»¤ë²„ë¦¬ì§€ â‰¥ 95%
- [ ] ì½”ë“œ ë¦¬ë·° ìŠ¹ì¸
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

### 10.2 ìŠ¹ì¸
- [ ] **Backend Lead**: _______________
- [ ] **Tech Lead**: _______________

**Review Deadline**: Task 1.7 ì™„ë£Œ í›„ 24ì‹œê°„ ì´ë‚´

---

**END OF EXECUTION PLAN**
