# Task 2.5a: LLM ê¸°ë³¸ ë‹µë³€ ìƒì„± - ì‹¤í–‰ ê³„íš

---

## ğŸ“‹ Meta

- **Task ID**: 2.5a
- **Taskëª…**: LLM ê¸°ë³¸ ë‹µë³€ ìƒì„±
- **ì˜ˆìƒ ì‹œê°„**: 4ì‹œê°„
- **ë‹´ë‹¹**: Backend
- **ì‘ì„±ì¼**: 2026-01-03
- **ìƒíƒœ**: Ready for Implementation
- **ë²„ì „**: 1.0.0

---

## 1. Executive Summary

### 1.1 ëª©í‘œ
LangChain RAG Chainì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. Ollama llama3ë¥¼ ìš°ì„  ì‚¬ìš©í•˜ë˜, í’ˆì§ˆì´ ë¶€ì¡±í•˜ë©´ OpenAIë¡œ ì „í™˜ ê°€ëŠ¥í•œ êµ¬ì¡°ë¡œ ì„¤ê³„í•©ë‹ˆë‹¤.

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­
- **ê¸°ëŠ¥**: LLM Provider ì¶”ìƒí™” (Ollama/OpenAI ì „í™˜ ê°€ëŠ¥)
- **í’ˆì§ˆ**: ìƒ˜í”Œ 5ê°œ ì§ˆë¬¸ìœ¼ë¡œ í’ˆì§ˆ í‰ê°€ (Ollama vs OpenAI ê²°ì •)
- **ì¶œì²˜**: [HARD RULE] ê²€ìƒ‰ëœ ë¬¸ì„œë§Œ ì‚¬ìš©, ì¶œì²˜ ëª…ì‹œ í•„ìˆ˜
- **ì„±ëŠ¥**: LLM í˜¸ì¶œ ì‹œê°„ < 25ì´ˆ

### 1.3 ì„±ê³µ ê¸°ì¤€
- [ ] Ollama llama3 ëª¨ë¸ ì •ìƒ ë™ì‘
- [ ] OpenAI GPT-4 ì—°ë™ êµ¬í˜„ (í™˜ê²½ ë³€ìˆ˜ ì „í™˜ë§Œìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥)
- [ ] RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
- [ ] ìƒ˜í”Œ 5ê°œ ì§ˆë¬¸ìœ¼ë¡œ í’ˆì§ˆ í‰ê°€ ì™„ë£Œ
- [ ] í’ˆì§ˆ í‰ê°€ ê²°ê³¼ ë¬¸ì„œí™”

### 1.4 Why This Task Matters
**RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ê¸°ëŠ¥**:
- **ë‹µë³€ í’ˆì§ˆ**: ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ìœ¼ë¡œ ë³€í™˜
- **ì¶œì²˜ ì‹ ë¢°ì„±**: ê²€ìƒ‰ëœ ë¬¸ì„œë§Œ ì‚¬ìš©í•˜ì—¬ Hallucination ë°©ì§€
- **ìœ ì—°ì„±**: LLM Provider êµì²´ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜

---

## 2. ì„ í–‰ ì¡°ê±´ ê²€ì¦

### 2.1 í™˜ê²½ ê²€ì¦
```bash
# Ollama llama3 ëª¨ë¸ í™•ì¸
ollama list | grep llama3

# LangChain ì„¤ì¹˜ í™•ì¸
python -c "import langchain; print(langchain.__version__)"

# OpenAI API Key í™•ì¸ (ì„ íƒì )
echo $OPENAI_API_KEY
```

### 2.2 ì˜ì¡´ì„± í™•ì¸
- [x] **Task 1.4**: Ollama llama3 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- [x] **Task 2.3**: VectorSearchService êµ¬í˜„ ì™„ë£Œ
- [ ] **requirements.txt**: langchain, langchain-community, openai

---

## 3. LLM Provider ì•„í‚¤í…ì²˜ ì„¤ê³„

### 3.1 Provider ì¶”ìƒí™”

```python
from abc import ABC, abstractmethod
from typing import List


class BaseLLMProvider(ABC):
    """LLM Provider ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤"""

    @abstractmethod
    def generate(self, prompt: str, context: str) -> str:
        """
        í”„ë¡¬í”„íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±

        Args:
            prompt: ì‚¬ìš©ì ì§ˆë¬¸
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©

        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        pass


class OllamaProvider(BaseLLMProvider):
    """Ollama LLM Provider (llama3)"""
    pass


class OpenAIProvider(BaseLLMProvider):
    """OpenAI LLM Provider (GPT-4)"""
    pass
```

### 3.2 Provider Factory íŒ¨í„´

```python
class LLMProviderFactory:
    """LLM Provider ìƒì„± íŒ©í† ë¦¬"""

    @staticmethod
    def create(provider_type: str = "ollama") -> BaseLLMProvider:
        if provider_type == "ollama":
            return OllamaProvider()
        elif provider_type == "openai":
            return OpenAIProvider()
        else:
            raise ValueError(f"Unknown provider: {provider_type}")
```

---

## 4. êµ¬í˜„ ë‹¨ê³„ë³„ ìƒì„¸ ê³„íš

### 4.1 Step 1: LLM Provider ì¶”ìƒí™” (60ë¶„)

#### ì‘ì—… ë‚´ìš©
**`backend/app/services/llm/base_provider.py` ì‘ì„±**:

```python
from abc import ABC, abstractmethod
from typing import List, Optional
from pydantic import BaseModel, Field
import logging

logger = logging.getLogger(__name__)


class LLMConfig(BaseModel):
    """LLM ì„¤ì •"""
    model_name: str = Field(..., description="ëª¨ë¸ëª…")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="ì˜¨ë„")
    max_tokens: int = Field(default=500, ge=50, le=2000, description="ìµœëŒ€ í† í°")
    timeout: int = Field(default=30, description="íƒ€ì„ì•„ì›ƒ (ì´ˆ)")


class BaseLLMProvider(ABC):
    """LLM Provider ì¶”ìƒ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self, config: LLMConfig):
        self.config = config
        logger.info(
            f"{self.__class__.__name__} ì´ˆê¸°í™”: "
            f"model={config.model_name}, "
            f"temperature={config.temperature}"
        )

    @abstractmethod
    def generate(self, prompt: str) -> str:
        """
        í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±

        Args:
            prompt: ì „ì²´ í”„ë¡¬í”„íŠ¸ (ì§ˆë¬¸ + ì»¨í…ìŠ¤íŠ¸ í¬í•¨)

        Returns:
            str: ìƒì„±ëœ ë‹µë³€

        Raises:
            LLMGenerationError: ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        pass

    @abstractmethod
    def health_check(self) -> bool:
        """Provider ìƒíƒœ í™•ì¸"""
        pass
```

---

### 4.2 Step 2: Ollama Provider êµ¬í˜„ (60ë¶„)

#### ì‘ì—… ë‚´ìš©
**`backend/app/services/llm/ollama_provider.py` ì‘ì„±**:

```python
import ollama
from app.services.llm.base_provider import BaseLLMProvider, LLMConfig
import logging

logger = logging.getLogger(__name__)


class OllamaProvider(BaseLLMProvider):
    """Ollama LLM Provider (llama3)"""

    def __init__(self, config: Optional[LLMConfig] = None):
        if config is None:
            config = LLMConfig(
                model_name="llama3",
                temperature=0.7,
                max_tokens=500,
                timeout=30
            )
        super().__init__(config)
        self.client = ollama.Client()

        # ëª¨ë¸ ì¡´ì¬ í™•ì¸
        if not self._verify_model_exists():
            raise ValueError(
                f"Ollama ëª¨ë¸ '{self.config.model_name}'ì´ ì—†ìŠµë‹ˆë‹¤. "
                f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: ollama pull {self.config.model_name}"
            )

    def _verify_model_exists(self) -> bool:
        """Ollama ëª¨ë¸ ì¡´ì¬ í™•ì¸"""
        try:
            response = self.client.list()
            model_names = [model.model for model in response.models]
            search_names = [
                self.config.model_name,
                f"{self.config.model_name}:latest"
            ]
            return any(name in model_names for name in search_names)
        except Exception as e:
            logger.error(f"Ollama ëª¨ë¸ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def generate(self, prompt: str) -> str:
        """
        Ollamaë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±

        Args:
            prompt: ì „ì²´ í”„ë¡¬í”„íŠ¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        try:
            logger.info(f"Ollama ë‹µë³€ ìƒì„± ì‹œì‘: prompt_length={len(prompt)}")

            response = self.client.generate(
                model=self.config.model_name,
                prompt=prompt,
                options={
                    "temperature": self.config.temperature,
                    "num_predict": self.config.max_tokens
                }
            )

            answer = response["response"].strip()

            logger.info(
                f"Ollama ë‹µë³€ ìƒì„± ì™„ë£Œ: answer_length={len(answer)}"
            )

            return answer

        except Exception as e:
            logger.error(f"Ollama ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            raise ValueError(f"LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")

    def health_check(self) -> bool:
        """Ollama ìƒíƒœ í™•ì¸"""
        return self._verify_model_exists()
```

---

### 4.3 Step 3: OpenAI Provider êµ¬í˜„ (60ë¶„)

#### ì‘ì—… ë‚´ìš©
**`backend/app/services/llm/openai_provider.py` ì‘ì„±**:

```python
import os
from openai import OpenAI
from app.services.llm.base_provider import BaseLLMProvider, LLMConfig
import logging

logger = logging.getLogger(__name__)


class OpenAIProvider(BaseLLMProvider):
    """OpenAI LLM Provider (GPT-4)"""

    def __init__(self, config: Optional[LLMConfig] = None):
        if config is None:
            config = LLMConfig(
                model_name="gpt-4",
                temperature=0.7,
                max_tokens=500,
                timeout=30
            )
        super().__init__(config)

        # OpenAI API Key í™•ì¸
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError(
                "OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
                ".env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì¶”ê°€í•˜ì„¸ìš”."
            )

        self.client = OpenAI(api_key=api_key)
        logger.info(f"OpenAI Provider ì´ˆê¸°í™”: model={self.config.model_name}")

    def generate(self, prompt: str) -> str:
        """
        OpenAIë¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±

        Args:
            prompt: ì „ì²´ í”„ë¡¬í”„íŠ¸

        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        try:
            logger.info(f"OpenAI ë‹µë³€ ìƒì„± ì‹œì‘: prompt_length={len(prompt)}")

            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
                timeout=self.config.timeout
            )

            answer = response.choices[0].message.content.strip()

            logger.info(
                f"OpenAI ë‹µë³€ ìƒì„± ì™„ë£Œ: answer_length={len(answer)}, "
                f"tokens={response.usage.total_tokens}"
            )

            return answer

        except Exception as e:
            logger.error(f"OpenAI ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            raise ValueError(f"LLM ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")

    def health_check(self) -> bool:
        """OpenAI API ìƒíƒœ í™•ì¸"""
        try:
            self.client.models.list()
            return True
        except Exception as e:
            logger.error(f"OpenAI health check ì‹¤íŒ¨: {e}")
            return False
```

---

### 4.4 Step 4: RAG ì„œë¹„ìŠ¤ êµ¬í˜„ (60ë¶„)

#### ì‘ì—… ë‚´ìš©
**`backend/app/services/rag_service.py` ì‘ì„±**:

```python
import os
from typing import List, Optional
from app.services.llm.base_provider import BaseLLMProvider
from app.services.llm.ollama_provider import OllamaProvider
from app.services.llm.openai_provider import OpenAIProvider
from app.services.vector_search import SearchResult
import logging

logger = logging.getLogger(__name__)


# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
RAG_PROMPT_TEMPLATE = """ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

[ë¬¸ì„œ]
{context}

[ì§ˆë¬¸]
{query}

[ê·œì¹™]
1. ë°˜ë“œì‹œ ìœ„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µí•˜ì§€ ë§ˆì„¸ìš”.
3. ë‹µë³€ì— ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš” (ì˜ˆ: "íœ´ê°€ ê·œì • ë¬¸ì„œì— ë”°ë¥´ë©´...").
4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•˜ì„¸ìš”.
5. ë‹µë³€ì€ 3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.

[ë‹µë³€]
"""


class RAGService:
    """RAG (Retrieval-Augmented Generation) ì„œë¹„ìŠ¤"""

    def __init__(self, provider_type: str = "ollama"):
        """
        Args:
            provider_type: "ollama" ë˜ëŠ” "openai"
        """
        self.provider_type = provider_type

        # LLM Provider ì´ˆê¸°í™”
        if provider_type == "ollama":
            self.llm_provider = OllamaProvider()
        elif provider_type == "openai":
            self.llm_provider = OpenAIProvider()
        else:
            raise ValueError(f"Unknown provider type: {provider_type}")

        logger.info(f"RAGService ì´ˆê¸°í™”: provider={provider_type}")

    def generate_answer(
        self,
        query: str,
        search_results: List[SearchResult]
    ) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€ ìƒì„±

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            search_results: ë²¡í„° ê²€ìƒ‰ ê²°ê³¼

        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        # Step 1: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ Fallback
        if not search_results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # Step 2: Context êµ¬ì„±
        context = self._build_context(search_results)

        # Step 3: í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = RAG_PROMPT_TEMPLATE.format(
            context=context,
            query=query
        )

        logger.info(
            f"RAG ë‹µë³€ ìƒì„± ì‹œì‘: query='{query[:50]}...', "
            f"context_length={len(context)}"
        )

        # Step 4: LLM ë‹µë³€ ìƒì„±
        try:
            answer = self.llm_provider.generate(prompt)

            logger.info(
                f"RAG ë‹µë³€ ìƒì„± ì™„ë£Œ: answer_length={len(answer)}"
            )

            return answer

        except Exception as e:
            logger.error(f"RAG ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _build_context(self, search_results: List[SearchResult]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            search_results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            str: ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´
        """
        context_parts = []

        for idx, result in enumerate(search_results, 1):
            doc_title = result.metadata.get("document_title", "Unknown")
            doc_source = result.metadata.get("document_source", "Unknown")
            page_num = result.page_number or "N/A"

            context_part = (
                f"[ë¬¸ì„œ {idx}] {doc_title}\n"
                f"ì¶œì²˜: {doc_source} (í˜ì´ì§€ {page_num})\n"
                f"ë‚´ìš©: {result.content}\n"
                f"ê´€ë ¨ë„: {result.relevance_score:.2f}\n"
            )

            context_parts.append(context_part)

        return "\n---\n".join(context_parts)
```

---

## 5. í’ˆì§ˆ í‰ê°€ ê³„íš

### 5.1 ìƒ˜í”Œ ì§ˆë¬¸ 5ê°œ

**`backend/tests/quality/sample_questions.py`**:

```python
SAMPLE_QUESTIONS = [
    {
        "id": "Q1",
        "question": "ì—°ì°¨ ì‚¬ìš© ë°©ë²•ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "expected_source": "íœ´ê°€ ê·œì • ë¬¸ì„œ"
    },
    {
        "id": "Q2",
        "question": "ê¸‰ì—¬ ì§€ê¸‰ì¼ì€ ì–¸ì œì¸ê°€ìš”?",
        "expected_source": "ê¸‰ì—¬ ê·œì • ë¬¸ì„œ"
    },
    {
        "id": "Q3",
        "question": "íšŒì˜ì‹¤ ì˜ˆì•½ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?",
        "expected_source": "ì‹œì„¤ ì´ìš© ì•ˆë‚´"
    },
    {
        "id": "Q4",
        "question": "ì¬íƒê·¼ë¬´ ì •ì±…ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤.",
        "expected_source": "ê·¼ë¬´ ê·œì • ë¬¸ì„œ"
    },
    {
        "id": "Q5",
        "question": "ê²½ì¡°ì‚¬ íœ´ê°€ëŠ” ë©°ì¹ ì¸ê°€ìš”?",
        "expected_source": "íœ´ê°€ ê·œì • ë¬¸ì„œ"
    }
]
```

### 5.2 í’ˆì§ˆ í‰ê°€ ê¸°ì¤€

| ê¸°ì¤€ | ë°°ì  | ì„¤ëª… |
|------|------|------|
| ì •í™•ë„ | 40ì  | ì§ˆë¬¸ì— ì •í™•í•œ ë‹µë³€ í¬í•¨ |
| ì¶œì²˜ ëª…ì‹œ | 30ì  | ë¬¸ì„œ ì¶œì²˜ ëª…í™•íˆ ì–¸ê¸‰ |
| ìœ ì°½ì„± | 20ì  | ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ |
| ê°„ê²°ì„± | 10ì  | 3-5ë¬¸ì¥ ì´ë‚´ |

### 5.3 í‰ê°€ ì‹¤í–‰

```bash
# Ollama í‰ê°€
pytest backend/tests/quality/test_llm_quality.py --provider=ollama -v

# OpenAI í‰ê°€
pytest backend/tests/quality/test_llm_quality.py --provider=openai -v
```

---

## 6. ê²€ì¦ ê¸°ì¤€

### 6.1 í•„ìˆ˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Ollama llama3 ë‹µë³€ ìƒì„± ì„±ê³µ
- [ ] OpenAI GPT-4 ë‹µë³€ ìƒì„± ì„±ê³µ
- [ ] í™˜ê²½ ë³€ìˆ˜ë§Œìœ¼ë¡œ Provider ì „í™˜ ê°€ëŠ¥
- [ ] RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
- [ ] ìƒ˜í”Œ 5ê°œ ì§ˆë¬¸ ë‹µë³€ ìƒì„±
- [ ] í’ˆì§ˆ í‰ê°€ ë¬¸ì„œ ì‘ì„±

### 6.2 í’ˆì§ˆ ê¸°ì¤€

- [ ] Ollama í‰ê·  ì ìˆ˜ â‰¥ 70ì  (100ì  ë§Œì )
- [ ] OpenAI í‰ê·  ì ìˆ˜ â‰¥ 80ì  (100ì  ë§Œì )

---

## 7. ì¶œë ¥ë¬¼

### 7.1 ìƒì„±ë  íŒŒì¼

1. `backend/app/services/llm/__init__.py`
2. `backend/app/services/llm/base_provider.py` - LLM Provider ì¶”ìƒ ì¸í„°í˜ì´ìŠ¤
3. `backend/app/services/llm/ollama_provider.py` - Ollama Provider
4. `backend/app/services/llm/openai_provider.py` - OpenAI Provider
5. `backend/app/services/rag_service.py` - RAG ì„œë¹„ìŠ¤
6. `backend/prompts/rag_prompt.txt` - í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
7. `backend/tests/quality/test_llm_quality.py` - í’ˆì§ˆ í‰ê°€ í…ŒìŠ¤íŠ¸
8. `docs/llm-quality-evaluation.md` - í’ˆì§ˆ í‰ê°€ ë¦¬í¬íŠ¸

### 7.2 ìˆ˜ì •ë  íŒŒì¼

1. `backend/requirements.txt` - openai íŒ¨í‚¤ì§€ ì¶”ê°€
2. `backend/.env.example` - OPENAI_API_KEY ì¶”ê°€

---

## 8. ì°¸ê³  ë¬¸ì„œ

- Task Breakdown: `docs/tasks/task-breakdown.md`
- LangChain RAG: https://python.langchain.com/docs/use_cases/question_answering/
- Ollama Python: https://github.com/ollama/ollama-python
- OpenAI API: https://platform.openai.com/docs/api-reference

---

**ì‘ì„±ì**: Claude Code (Sonnet 4.5)
**ì‘ì„±ì¼**: 2026-01-03
