# Task 4.1 ì‹¤í–‰ ê³„íš: ë¬¸ì„œ ìë™ ì¸ë±ì‹± ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬

## ğŸ“‹ ì‘ì—… ì •ë³´
- **Task ID**: 4.1
- **Taskëª…**: ë¬¸ì„œ ìë™ ì¸ë±ì‹± ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬
- **ì˜ˆìƒ ì‹œê°„**: 6ì‹œê°„
- **ë‹´ë‹¹**: Backend
- **ì˜ì¡´ì„±**: Task 1.8 (ë¬¸ì„œ ì„ë² ë”© ë° Milvus ì €ì¥)
- **GitHub Issue**: #30

---

## ğŸ¯ ì‘ì—… ëª©í‘œ

ì‹ ê·œ ë¬¸ì„œë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê³  ì¸ë±ì‹±í•˜ëŠ” ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ êµ¬í˜„í•˜ì—¬ ë¬¸ì„œ ê´€ë¦¬ ìë™í™”

---

## ğŸ“ ê¸°ìˆ  ìŠ¤íƒ

- **Python**: 3.11+
- **APScheduler**: 3.10+ (ì‘ì—… ìŠ¤ì¼€ì¤„ë§)
- **FastAPI**: 0.100+ (ìˆ˜ë™ íŠ¸ë¦¬ê±° API)
- **SQLAlchemy**: 2.0+ (ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê´€ë¦¬)
- **Watchdog**: 3.0+ (íŒŒì¼ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§, ì„ íƒì‚¬í•­)

---

## ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Batch Scheduler System                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  APScheduler     â”‚â”€â”€â”€â”€â”€â”€â”‚  File Scanner    â”‚            â”‚
â”‚  â”‚  (Cron: 2AM)     â”‚      â”‚  (í´ë” ëª¨ë‹ˆí„°ë§)   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚           â”‚                         â”‚                        â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                     â–¼                                        â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚           â”‚  Indexing Queue  â”‚                              â”‚
â”‚           â”‚  (ì‹ ê·œ ë¬¸ì„œ í)    â”‚                              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                     â”‚                                        â”‚
â”‚                     â–¼                                        â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚           â”‚  Document Indexerâ”‚â—„â”€â”€â”€ Task 1.8 ì¬ì‚¬ìš©           â”‚
â”‚           â”‚  (íŒŒì‹± + ì„ë² ë”©)   â”‚                              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                     â”‚                                        â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
â”‚           â–¼                   â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚  PostgreSQL   â”‚   â”‚    Milvus     â”‚                     â”‚
â”‚  â”‚  (ë©”íƒ€ë°ì´í„°)   â”‚   â”‚   (ë²¡í„°)      â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚  Admin API (ìˆ˜ë™ íŠ¸ë¦¬ê±°)                   â”‚               â”‚
â”‚  â”‚  POST /api/v1/admin/index                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ êµ¬í˜„ ê³„íš

### Phase 1: APScheduler ì„¤ì • (2ì‹œê°„)

#### 1.1 APScheduler ì„¤ì¹˜ ë° ì„¤ì •
**íŒŒì¼**: `backend/requirements.txt`
```python
apscheduler==3.10.4
```

**íŒŒì¼**: `backend/app/scheduler/config.py`
```python
"""
ë°°ì¹˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
"""
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)


def create_scheduler() -> AsyncIOScheduler:
    """
    APScheduler ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

    Returns:
        AsyncIOScheduler: ì„¤ì •ëœ ìŠ¤ì¼€ì¤„ëŸ¬
    """
    jobstores = {
        'default': SQLAlchemyJobStore(url=settings.DATABASE_URL)
    }

    job_defaults = {
        'coalesce': True,  # ëˆ„ë½ëœ ì‹¤í–‰ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
        'max_instances': 1,  # ë™ì‹œ ì‹¤í–‰ ë°©ì§€
        'misfire_grace_time': 3600  # 1ì‹œê°„ ì´ë‚´ ëˆ„ë½ ì‹¤í–‰ í—ˆìš©
    }

    scheduler = AsyncIOScheduler(
        jobstores=jobstores,
        job_defaults=job_defaults,
        timezone='Asia/Seoul'
    )

    logger.info("APScheduler initialized")
    return scheduler
```

#### 1.2 ìŠ¤ì¼€ì¤„ëŸ¬ í†µí•©
**íŒŒì¼**: `backend/app/main.py`
```python
from contextlib import asynccontextmanager
from app.scheduler.config import create_scheduler
from app.scheduler.jobs import register_jobs

scheduler = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰"""
    global scheduler

    # Startup
    logger.info("FastAPI ì„œë²„ ì‹œì‘")

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    scheduler = create_scheduler()
    register_jobs(scheduler)
    scheduler.start()
    logger.info("Scheduler started")

    yield

    # Shutdown
    if scheduler:
        scheduler.shutdown()
        logger.info("Scheduler stopped")

    logger.info("FastAPI ì„œë²„ ì¢…ë£Œ")
```

---

### Phase 2: ë¬¸ì„œ ìŠ¤ìº” ë¡œì§ êµ¬í˜„ (2ì‹œê°„)

#### 2.1 íŒŒì¼ ìŠ¤ìºë„ˆ
**íŒŒì¼**: `backend/app/scheduler/file_scanner.py`
```python
"""
ë¬¸ì„œ ì €ì¥ì†Œ ìŠ¤ìº” ë° ì‹ ê·œ ë¬¸ì„œ ê°ì§€
"""
from pathlib import Path
from typing import List, Dict
from sqlalchemy.ext.asyncio import AsyncSession
from app.models.document import Document
import logging

logger = logging.getLogger(__name__)


class FileScanner:
    """ë¬¸ì„œ ì €ì¥ì†Œ ìŠ¤ìºë„ˆ"""

    SUPPORTED_EXTENSIONS = {'.pdf', '.docx', '.txt', '.md'}

    def __init__(self, watch_dir: str):
        """
        Args:
            watch_dir: ëª¨ë‹ˆí„°ë§í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.watch_dir = Path(watch_dir)
        if not self.watch_dir.exists():
            raise ValueError(f"Directory not found: {watch_dir}")

    async def scan_for_new_documents(
        self,
        db: AsyncSession
    ) -> List[Dict[str, str]]:
        """
        ì‹ ê·œ ë¬¸ì„œ ìŠ¤ìº”

        Args:
            db: DB ì„¸ì…˜

        Returns:
            List[Dict]: ì‹ ê·œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
                - file_path: íŒŒì¼ ê²½ë¡œ
                - file_name: íŒŒì¼ëª…
                - file_type: íŒŒì¼ íƒ€ì…
        """
        new_docs = []

        # ì €ì¥ì†Œ ì „ì²´ ìŠ¤ìº”
        for file_path in self.watch_dir.rglob('*'):
            if not file_path.is_file():
                continue

            if file_path.suffix.lower() not in self.SUPPORTED_EXTENSIONS:
                continue

            # DBì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            exists = await self._check_document_exists(
                db,
                str(file_path.absolute())
            )

            if not exists:
                new_docs.append({
                    'file_path': str(file_path.absolute()),
                    'file_name': file_path.name,
                    'file_type': file_path.suffix.lower()[1:]  # .pdf -> pdf
                })

        logger.info(f"Found {len(new_docs)} new documents")
        return new_docs

    async def _check_document_exists(
        self,
        db: AsyncSession,
        file_path: str
    ) -> bool:
        """
        ë¬¸ì„œê°€ DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸

        Args:
            db: DB ì„¸ì…˜
            file_path: íŒŒì¼ ê²½ë¡œ

        Returns:
            bool: ì¡´ì¬ ì—¬ë¶€
        """
        from sqlalchemy import select

        stmt = select(Document).where(
            Document.document_source == file_path
        )
        result = await db.execute(stmt)
        return result.scalar_one_or_none() is not None
```

#### 2.2 ì¸ë±ì‹± í ê´€ë¦¬
**íŒŒì¼**: `backend/app/scheduler/indexing_queue.py`
```python
"""
ì¸ë±ì‹± ì‘ì—… í ê´€ë¦¬
"""
from typing import List, Dict
import asyncio
from app.services.document_indexer import DocumentIndexer
from app.db.base import get_db
import logging

logger = logging.getLogger(__name__)


class IndexingQueue:
    """ì¸ë±ì‹± ì‘ì—… í"""

    def __init__(self, max_concurrent: int = 5):
        """
        Args:
            max_concurrent: ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜
        """
        self.max_concurrent = max_concurrent
        self.indexer = DocumentIndexer()

    async def process_documents(
        self,
        documents: List[Dict[str, str]]
    ) -> Dict[str, int]:
        """
        ë¬¸ì„œ ë°°ì¹˜ ì¸ë±ì‹±

        Args:
            documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
                - success: ì„±ê³µ ê°œìˆ˜
                - failed: ì‹¤íŒ¨ ê°œìˆ˜
                - total: ì „ì²´ ê°œìˆ˜
        """
        if not documents:
            return {'success': 0, 'failed': 0, 'total': 0}

        results = {
            'success': 0,
            'failed': 0,
            'total': len(documents)
        }

        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def process_with_semaphore(doc: Dict[str, str]):
            async with semaphore:
                return await self._process_single_document(doc)

        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = [
            process_with_semaphore(doc)
            for doc in documents
        ]

        completed = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì§‘ê³„
        for result in completed:
            if isinstance(result, Exception):
                results['failed'] += 1
                logger.error(f"Indexing failed: {result}")
            elif result:
                results['success'] += 1
            else:
                results['failed'] += 1

        logger.info(
            f"Indexing completed: {results['success']}/{results['total']} succeeded"
        )

        return results

    async def _process_single_document(
        self,
        doc: Dict[str, str]
    ) -> bool:
        """
        ë‹¨ì¼ ë¬¸ì„œ ì¸ë±ì‹± (ì¬ì‹œë„ í¬í•¨)

        Args:
            doc: ë¬¸ì„œ ì •ë³´

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                async for db in get_db():
                    await self.indexer.index_document(
                        db=db,
                        file_path=doc['file_path'],
                        file_name=doc['file_name'],
                        file_type=doc['file_type']
                    )
                    return True

            except Exception as e:
                retry_count += 1
                logger.warning(
                    f"Indexing retry {retry_count}/{max_retries}: {e}"
                )

                if retry_count >= max_retries:
                    logger.error(
                        f"Indexing failed after {max_retries} retries: "
                        f"{doc['file_path']}"
                    )
                    return False

                # Exponential backoff
                await asyncio.sleep(2 ** retry_count)

        return False
```

---

### Phase 3: ìŠ¤ì¼€ì¤„ ì‘ì—… ë“±ë¡ (1ì‹œê°„)

#### 3.1 ë°°ì¹˜ ì‘ì—… ì •ì˜
**íŒŒì¼**: `backend/app/scheduler/jobs.py`
```python
"""
ìŠ¤ì¼€ì¤„ ì‘ì—… ì •ì˜
"""
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from app.scheduler.file_scanner import FileScanner
from app.scheduler.indexing_queue import IndexingQueue
from app.db.base import get_db
from app.core.config import settings
import logging

logger = logging.getLogger(__name__)


async def auto_index_new_documents():
    """
    ì‹ ê·œ ë¬¸ì„œ ìë™ ì¸ë±ì‹± ì‘ì—…

    ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì‹¤í–‰
    """
    logger.info("Starting auto-indexing job")

    try:
        scanner = FileScanner(settings.DOCUMENT_STORAGE_PATH)
        queue = IndexingQueue(max_concurrent=5)

        async for db in get_db():
            # ì‹ ê·œ ë¬¸ì„œ ìŠ¤ìº”
            new_docs = await scanner.scan_for_new_documents(db)

            if not new_docs:
                logger.info("No new documents found")
                return

            # ì¸ë±ì‹± ì‹¤í–‰
            results = await queue.process_documents(new_docs)

            logger.info(
                f"Auto-indexing completed: "
                f"{results['success']}/{results['total']} succeeded, "
                f"{results['failed']} failed"
            )

    except Exception as e:
        logger.error(f"Auto-indexing job failed: {e}", exc_info=True)
        raise


def register_jobs(scheduler: AsyncIOScheduler):
    """
    ìŠ¤ì¼€ì¤„ ì‘ì—… ë“±ë¡

    Args:
        scheduler: APScheduler ì¸ìŠ¤í„´ìŠ¤
    """
    # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ìë™ ì¸ë±ì‹±
    scheduler.add_job(
        auto_index_new_documents,
        trigger=CronTrigger(hour=2, minute=0),
        id='auto_index_documents',
        name='Auto Index New Documents',
        replace_existing=True
    )

    logger.info("Jobs registered successfully")
```

---

### Phase 4: ìˆ˜ë™ íŠ¸ë¦¬ê±° API (1ì‹œê°„)

#### 4.1 ê´€ë¦¬ì API ë¼ìš°í„°
**íŒŒì¼**: `backend/app/routers/admin.py`
```python
"""
ê´€ë¦¬ì API ì—”ë“œí¬ì¸íŠ¸
"""
from fastapi import APIRouter, Depends, HTTPException, status, BackgroundTasks
from pydantic import BaseModel
from typing import Dict, Any
from app.scheduler.file_scanner import FileScanner
from app.scheduler.indexing_queue import IndexingQueue
from app.db.base import get_db
from app.core.config import settings
from app.routers.auth import get_current_user
from sqlalchemy.ext.asyncio import AsyncSession
import logging

logger = logging.getLogger(__name__)
router = APIRouter(prefix="/admin", tags=["admin"])


class IndexTriggerResponse(BaseModel):
    """ì¸ë±ì‹± íŠ¸ë¦¬ê±° ì‘ë‹µ"""
    message: str
    job_id: str


class IndexStatusResponse(BaseModel):
    """ì¸ë±ì‹± ìƒíƒœ ì‘ë‹µ"""
    status: str  # "pending", "running", "completed", "failed"
    total: int
    success: int
    failed: int
    started_at: str | None
    completed_at: str | None


async def verify_admin_user(
    user: Dict[str, Any] = Depends(get_current_user)
) -> Dict[str, Any]:
    """
    ê´€ë¦¬ì ê¶Œí•œ í™•ì¸

    Args:
        user: í˜„ì¬ ì‚¬ìš©ì

    Returns:
        Dict: ì‚¬ìš©ì ì •ë³´

    Raises:
        HTTPException 403: ê´€ë¦¬ì ê¶Œí•œ ì—†ìŒ
    """
    if user.get("access_level", 0) < 3:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="ê´€ë¦¬ì ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤"
        )

    return user


@router.post("/index", response_model=IndexTriggerResponse)
async def trigger_manual_indexing(
    background_tasks: BackgroundTasks,
    db: AsyncSession = Depends(get_db),
    user: Dict[str, Any] = Depends(verify_admin_user)
) -> IndexTriggerResponse:
    """
    ìˆ˜ë™ ì¸ë±ì‹± íŠ¸ë¦¬ê±°

    ê´€ë¦¬ìë§Œ ì‹¤í–‰ ê°€ëŠ¥

    Args:
        background_tasks: FastAPI ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…
        db: DB ì„¸ì…˜
        user: í˜„ì¬ ì‚¬ìš©ì (ê´€ë¦¬ì)

    Returns:
        IndexTriggerResponse: ì‘ì—… ID ë° ë©”ì‹œì§€
    """
    import uuid

    job_id = f"manual_index_{uuid.uuid4().hex[:8]}"

    logger.info(
        f"Manual indexing triggered: job_id={job_id}, "
        f"user={user['email']}"
    )

    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¸ë±ì‹± ì‹¤í–‰
    background_tasks.add_task(
        run_manual_indexing,
        job_id=job_id,
        user_email=user['email']
    )

    return IndexTriggerResponse(
        message="ì¸ë±ì‹± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤",
        job_id=job_id
    )


async def run_manual_indexing(job_id: str, user_email: str):
    """
    ìˆ˜ë™ ì¸ë±ì‹± ì‹¤í–‰ (ë°±ê·¸ë¼ìš´ë“œ)

    Args:
        job_id: ì‘ì—… ID
        user_email: ì‚¬ìš©ì ì´ë©”ì¼
    """
    try:
        logger.info(f"Starting manual indexing: job_id={job_id}")

        scanner = FileScanner(settings.DOCUMENT_STORAGE_PATH)
        queue = IndexingQueue(max_concurrent=5)

        async for db in get_db():
            new_docs = await scanner.scan_for_new_documents(db)

            if not new_docs:
                logger.info(f"No new documents: job_id={job_id}")
                return

            results = await queue.process_documents(new_docs)

            logger.info(
                f"Manual indexing completed: job_id={job_id}, "
                f"success={results['success']}, failed={results['failed']}"
            )

    except Exception as e:
        logger.error(
            f"Manual indexing failed: job_id={job_id}, error={e}",
            exc_info=True
        )
```

#### 4.2 main.pyì— ë¼ìš°í„° ë“±ë¡
**íŒŒì¼**: `backend/app/main.py`
```python
from app.routers import admin

# ë¼ìš°í„° ë“±ë¡
app.include_router(admin.router, prefix="/api/v1", tags=["Admin"])
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ê³„íš

### ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
**íŒŒì¼**: `backend/tests/test_scheduler.py`
```python
import pytest
from app.scheduler.file_scanner import FileScanner
from app.scheduler.indexing_queue import IndexingQueue


class TestFileScanner:
    """íŒŒì¼ ìŠ¤ìºë„ˆ í…ŒìŠ¤íŠ¸"""

    @pytest.mark.asyncio
    async def test_scan_new_documents(self, db_session, tmp_path):
        """ì‹ ê·œ ë¬¸ì„œ ìŠ¤ìº” í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±
        test_file = tmp_path / "test.pdf"
        test_file.write_text("test content")

        scanner = FileScanner(str(tmp_path))
        new_docs = await scanner.scan_for_new_documents(db_session)

        assert len(new_docs) == 1
        assert new_docs[0]['file_name'] == 'test.pdf'
        assert new_docs[0]['file_type'] == 'pdf'

    @pytest.mark.asyncio
    async def test_ignore_unsupported_files(self, db_session, tmp_path):
        """ë¯¸ì§€ì› íŒŒì¼ ë¬´ì‹œ í…ŒìŠ¤íŠ¸"""
        test_file = tmp_path / "test.xyz"
        test_file.write_text("test content")

        scanner = FileScanner(str(tmp_path))
        new_docs = await scanner.scan_for_new_documents(db_session)

        assert len(new_docs) == 0


class TestIndexingQueue:
    """ì¸ë±ì‹± í í…ŒìŠ¤íŠ¸"""

    @pytest.mark.asyncio
    async def test_process_documents_success(self, mocker):
        """ë¬¸ì„œ ì²˜ë¦¬ ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        # Mock indexer
        mock_indexer = mocker.patch(
            'app.scheduler.indexing_queue.DocumentIndexer'
        )

        queue = IndexingQueue()
        docs = [
            {'file_path': '/test1.pdf', 'file_name': 'test1.pdf', 'file_type': 'pdf'}
        ]

        results = await queue.process_documents(docs)

        assert results['total'] == 1
        assert results['success'] == 1
        assert results['failed'] == 0

    @pytest.mark.asyncio
    async def test_process_documents_with_retry(self, mocker):
        """ì¬ì‹œë„ ë¡œì§ í…ŒìŠ¤íŠ¸"""
        # First call fails, second succeeds
        mock_indexer = mocker.patch(
            'app.scheduler.indexing_queue.DocumentIndexer.index_document',
            side_effect=[Exception("Error"), None]
        )

        queue = IndexingQueue()
        doc = {'file_path': '/test.pdf', 'file_name': 'test.pdf', 'file_type': 'pdf'}

        result = await queue._process_single_document(doc)

        assert result is True
        assert mock_indexer.call_count == 2
```

### í†µí•© í…ŒìŠ¤íŠ¸
**íŒŒì¼**: `backend/tests/integration/test_batch_scheduler.py`
```python
import pytest
from httpx import AsyncClient


@pytest.mark.asyncio
async def test_manual_indexing_trigger(client: AsyncClient, admin_token):
    """ìˆ˜ë™ ì¸ë±ì‹± íŠ¸ë¦¬ê±° í…ŒìŠ¤íŠ¸"""
    response = await client.post(
        "/api/v1/admin/index",
        headers={"Authorization": f"Bearer {admin_token}"}
    )

    assert response.status_code == 200
    data = response.json()
    assert data['message'] == "ì¸ë±ì‹± ì‘ì—…ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤"
    assert 'job_id' in data


@pytest.mark.asyncio
async def test_manual_indexing_requires_admin(client: AsyncClient, user_token):
    """ê´€ë¦¬ì ê¶Œí•œ í•„ìš” í…ŒìŠ¤íŠ¸"""
    response = await client.post(
        "/api/v1/admin/index",
        headers={"Authorization": f"Bearer {user_token}"}
    )

    assert response.status_code == 403
```

---

## âœ… ê²€ì¦ ê¸°ì¤€

### ê¸°ëŠ¥ ê²€ì¦
- [ ] APScheduler ì •ìƒ ì‹¤í–‰ (ì„œë²„ ì‹œì‘ ì‹œ)
- [ ] ì‹ ê·œ ë¬¸ì„œ ìŠ¤ìº” ì„±ê³µ (í…ŒìŠ¤íŠ¸ íŒŒì¼ 5ê°œ)
- [ ] ìë™ ì¸ë±ì‹± ìŠ¤ì¼€ì¤„ ë“±ë¡ í™•ì¸ (ìƒˆë²½ 2ì‹œ)
- [ ] ìˆ˜ë™ íŠ¸ë¦¬ê±° API í…ŒìŠ¤íŠ¸ (POST /api/v1/admin/index)
- [ ] ê´€ë¦¬ì ê¶Œí•œ í™•ì¸ (ì¼ë°˜ ì‚¬ìš©ì 403)

### ì„±ëŠ¥ ê²€ì¦
- [ ] 10ê°œ ë¬¸ì„œ ë™ì‹œ ì¸ë±ì‹± ì„±ê³µ
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ì¸¡ì • (10ê°œ < 5ë¶„)
- [ ] ì¬ì‹œë„ ë¡œì§ í™•ì¸ (ì‹¤íŒ¨ â†’ ì¬ì‹œë„ 3íšŒ)

### ë¡œê·¸ ê²€ì¦
- [ ] ë°°ì¹˜ ì‹œì‘/ì¢…ë£Œ ë¡œê·¸ í™•ì¸
- [ ] ì„±ê³µ/ì‹¤íŒ¨ ê°œìˆ˜ ë¡œê·¸ í™•ì¸
- [ ] ì—ëŸ¬ ë¡œê·¸ í™•ì¸ (ì‹¤íŒ¨ ì‹œ)

---

## ğŸ“‚ íŒŒì¼ êµ¬ì¡°

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ scheduler/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py              # APScheduler ì„¤ì •
â”‚   â”‚   â”œâ”€â”€ jobs.py                # ìŠ¤ì¼€ì¤„ ì‘ì—… ì •ì˜
â”‚   â”‚   â”œâ”€â”€ file_scanner.py        # ë¬¸ì„œ ìŠ¤ìº”
â”‚   â”‚   â””â”€â”€ indexing_queue.py      # ì¸ë±ì‹± í
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â””â”€â”€ admin.py               # ê´€ë¦¬ì API
â”‚   â””â”€â”€ main.py                     # ìŠ¤ì¼€ì¤„ëŸ¬ í†µí•©
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_scheduler.py          # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_batch_scheduler.py # í†µí•© í…ŒìŠ¤íŠ¸
â””â”€â”€ requirements.txt               # apscheduler ì¶”ê°€
```

---

## ğŸ”’ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### [HARD RULE] ì¤€ìˆ˜
1. **ê´€ë¦¬ì ê¶Œí•œ í™•ì¸**
   - ìˆ˜ë™ íŠ¸ë¦¬ê±° APIëŠ” access_level >= 3ë§Œ í—ˆìš©
   - JWT í† í° ê²€ì¦ í•„ìˆ˜

2. **íŒŒì¼ ê²½ë¡œ ê²€ì¦**
   - ì‹¬ë³¼ë¦­ ë§í¬ ì°¨ë‹¨
   - ì ˆëŒ€ ê²½ë¡œë§Œ í—ˆìš©
   - ë””ë ‰í† ë¦¬ íŠ¸ë˜ë²„ì„¤ ë°©ì§€

3. **ë¡œê·¸ ë³´ì•ˆ**
   - íŒŒì¼ ê²½ë¡œì— ë¯¼ê° ì •ë³´ í¬í•¨ ì‹œ ë§ˆìŠ¤í‚¹
   - ì—ëŸ¬ ë¡œê·¸ì— ì‹œìŠ¤í…œ ì •ë³´ ë…¸ì¶œ ê¸ˆì§€

### ì…ë ¥ ê²€ì¦
```python
# íŒŒì¼ ê²½ë¡œ ê²€ì¦
def validate_file_path(file_path: Path) -> bool:
    """
    íŒŒì¼ ê²½ë¡œ ì•ˆì „ì„± ê²€ì¦

    Args:
        file_path: ê²€ì¦í•  ê²½ë¡œ

    Returns:
        bool: ì•ˆì „ ì—¬ë¶€
    """
    # ì‹¬ë³¼ë¦­ ë§í¬ ì°¨ë‹¨
    if file_path.is_symlink():
        return False

    # ì ˆëŒ€ ê²½ë¡œ í™•ì¸
    if not file_path.is_absolute():
        return False

    # watch_dir í•˜ìœ„ ê²½ë¡œì¸ì§€ í™•ì¸
    try:
        file_path.relative_to(WATCH_DIR)
    except ValueError:
        return False

    return True
```

---

## ğŸš¨ ì—ëŸ¬ ì²˜ë¦¬

### ì¬ì‹œë„ ì „ëµ
```python
# Exponential Backoff
retry_delays = [1, 2, 4]  # 1ì´ˆ, 2ì´ˆ, 4ì´ˆ

for attempt in range(3):
    try:
        await index_document(doc)
        break
    except Exception as e:
        if attempt < 2:
            await asyncio.sleep(retry_delays[attempt])
        else:
            logger.error(f"Failed after 3 retries: {doc}")
```

### ì—ëŸ¬ ë¶„ë¥˜
1. **Retriable Errors** (ì¬ì‹œë„ ê°€ëŠ¥)
   - ë„¤íŠ¸ì›Œí¬ íƒ€ì„ì•„ì›ƒ
   - Milvus ì—°ê²° ì‹¤íŒ¨
   - DB ë½

2. **Non-Retriable Errors** (ì¬ì‹œë„ ë¶ˆê°€)
   - íŒŒì¼ ì†ìƒ
   - íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜
   - ê¶Œí•œ ì—†ìŒ

---

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### ë°°ì¹˜ ì‹¤í–‰ ë¡œê·¸
```json
{
  "timestamp": "2026-01-10T02:00:00Z",
  "job_id": "auto_index_documents",
  "status": "completed",
  "duration_seconds": 120,
  "documents_scanned": 15,
  "documents_indexed": 12,
  "documents_failed": 3,
  "success_rate": 0.8
}
```

### ì•Œë¦¼ ì¡°ê±´
- ì„±ê³µë¥  < 80% â†’ ê²½ê³ 
- ë°°ì¹˜ ì‹¤í–‰ ì‹¤íŒ¨ â†’ ì—ëŸ¬
- ì²˜ë¦¬ ì‹œê°„ > 30ë¶„ â†’ ê²½ê³ 

---

## ğŸ”„ í–¥í›„ ê°œì„  ì‚¬í•­

### Phase 4 ì´í›„
1. **ì‹¤ì‹œê°„ íŒŒì¼ ëª¨ë‹ˆí„°ë§**
   - Watchdog ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‹¤ì‹œê°„ ê°ì§€
   - íŒŒì¼ ìƒì„± ì¦‰ì‹œ ì¸ë±ì‹±

2. **ì¦ë¶„ ì—…ë°ì´íŠ¸**
   - ìˆ˜ì •ëœ ë¬¸ì„œ ì¬ì¸ë±ì‹±
   - ë³€ê²½ ê°ì§€ (íŒŒì¼ í•´ì‹œ)

3. **ë°°ì¹˜ ìš°ì„ ìˆœìœ„**
   - ì¤‘ìš” ë¬¸ì„œ ìš°ì„  ì²˜ë¦¬
   - ë¶€ì„œë³„ ìš°ì„ ìˆœìœ„

4. **ë¶„ì‚° ì²˜ë¦¬**
   - Celeryë¡œ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬
   - Redis í

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [APScheduler Documentation](https://apscheduler.readthedocs.io/)
- [FastAPI Background Tasks](https://fastapi.tiangolo.com/tutorial/background-tasks/)
- [Watchdog Documentation](https://python-watchdog.readthedocs.io/)

---

**ì‘ì„±ì**: Task Planner
**ì‘ì„±ì¼**: 2026-01-10
**ë²„ì „**: 1.0.0
