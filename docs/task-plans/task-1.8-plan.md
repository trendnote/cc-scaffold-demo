# Task 1.8: ë¬¸ì„œ ì„ë² ë”© ë° Milvus ì €ì¥ - ì‹¤í–‰ ê³„íš

---

## ğŸ“‹ Meta

- **Task ID**: 1.8
- **Taskëª…**: ë¬¸ì„œ ì„ë² ë”© ë° Milvus ì €ì¥
- **ì˜ˆìƒ ì‹œê°„**: 6ì‹œê°„
- **ë‹´ë‹¹**: Backend
- **ì‘ì„±ì¼**: 2026-01-02
- **ìƒíƒœ**: Ready for Implementation
- **ë²„ì „**: 1.0.0

---

## 1. Executive Summary

### 1.1 ëª©í‘œ
ì²­í¬ë¡œ ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ Ollama nomic-embed-text ëª¨ë¸ë¡œ ì„ë² ë”©í•˜ê³ , Milvus ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ PostgreSQLì— ì €ì¥í•˜ì—¬ **ê²€ìƒ‰ ê°€ëŠ¥í•œ ì¸ë±ìŠ¤**ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. Phase 1ì˜ ìµœì¢… ë‹¨ê³„ë¡œ **ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ì„±**ì´ ëª©í‘œì…ë‹ˆë‹¤.

### 1.2 í•µì‹¬ ìš”êµ¬ì‚¬í•­
- **ê¸°ëŠ¥**: ì²­í¬ ì„ë² ë”© ìƒì„± â†’ Milvus ì €ì¥ â†’ PostgreSQL ë©”íƒ€ë°ì´í„° ì €ì¥
- **ì„±ëŠ¥**: ë°°ì¹˜ ì²˜ë¦¬ (5ê°œ ë¬¸ì„œ ë³‘ë ¬), ì¬ì‹œë„ ë¡œì§ (3íšŒ)
- **ì•ˆì •ì„±**: ì—ëŸ¬ í•¸ë“¤ë§, ë¡¤ë°±, íŠ¸ëœì­ì…˜ ê´€ë¦¬
- **ê²€ì¦**: 10ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹± ì„±ê³µ, ì„ë² ë”© ì°¨ì› 768 í™•ì¸

### 1.3 ì„±ê³µ ê¸°ì¤€
- [ ] 10ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹± ì„±ê³µ
- [ ] Attu UIì—ì„œ ë²¡í„° í™•ì¸ (768ì°¨ì›)
- [ ] PostgreSQLì— ë©”íƒ€ë°ì´í„° ì €ì¥ í™•ì¸
- [ ] ì„ë² ë”© ì°¨ì› ê²€ì¦ (768ì°¨ì›)
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì •ìƒ ë™ì‘ (5ê°œ ë³‘ë ¬)
- [ ] ì¬ì‹œë„ ë¡œì§ ë™ì‘ í™•ì¸
- [ ] ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦

### 1.4 Why This Task Matters
**RAG ì‹œìŠ¤í…œì˜ í•µì‹¬ ì™„ì„±**:
- **ê²€ìƒ‰ ê°€ëŠ¥ì„±**: ë²¡í„° ì„ë² ë”© â†’ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰
- **ì „ì²´ íŒŒì´í”„ë¼ì¸ ì—°ê²°**: íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥
- **Phase 2 ì¤€ë¹„**: ê²€ìƒ‰ API êµ¬í˜„ ê¸°ë°˜ ë§ˆë ¨
- **ë°ì´í„° ë¬´ê²°ì„±**: PostgreSQL + Milvus ì´ì¤‘ ì €ì¥

---

## 2. ì„ í–‰ ì¡°ê±´ ê²€ì¦

### 2.1 í™˜ê²½ ê²€ì¦
ì‹¤í–‰ ì „ ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•©ë‹ˆë‹¤:

```bash
# Python ë²„ì „ í™•ì¸ (3.11+ í•„ìš”)
python --version

# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
which python  # venv ê²½ë¡œì—¬ì•¼ í•¨

# Task 1.1-1.7 ì™„ë£Œ í™•ì¸
ls -la app/services/document_parser/
ls -la app/services/text_chunker.py
ls -la app/db/milvus_client.py
ls -la app/models/

# Docker ì»¨í…Œì´ë„ˆ ìƒíƒœ í™•ì¸
docker ps | grep milvus
docker ps | grep postgres
docker ps | grep ollama
```

### 2.2 ì„œë¹„ìŠ¤ í™•ì¸
```bash
# Milvus ì—°ê²° í™•ì¸
python -c "from pymilvus import connections; connections.connect('default', host='localhost', port='19530'); print('Milvus OK')"

# PostgreSQL ì—°ê²° í™•ì¸
psql -h localhost -U postgres -d rag_platform -c "SELECT 1;"

# Ollama ëª¨ë¸ í™•ì¸
docker exec -it ollama ollama list | grep nomic-embed-text
```

### 2.3 ì˜ì¡´ì„± í™•ì¸
ë‹¤ìŒ Taskë“¤ì´ ì™„ë£Œë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤:

- [x] **Task 1.3**: Milvus Collection ìƒì„± ì™„ë£Œ
- [x] **Task 1.4**: Ollama nomic-embed-text ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- [x] **Task 1.5**: PDF íŒŒì„œ êµ¬í˜„ ì™„ë£Œ
- [x] **Task 1.6**: DOCX, TXT, Markdown íŒŒì„œ êµ¬í˜„ ì™„ë£Œ
- [x] **Task 1.7**: í…ìŠ¤íŠ¸ ì²­ì»¤ êµ¬í˜„ ì™„ë£Œ
- [x] **Task 1.2**: PostgreSQL ìŠ¤í‚¤ë§ˆ ë° Documents í…Œì´ë¸” ìƒì„± ì™„ë£Œ

---

## 3. ê¸°ìˆ  ìŠ¤íƒ ì„ íƒ

### 3.1 ì„ë² ë”© ëª¨ë¸ ì„ íƒ

| ëª¨ë¸ | ì°¨ì› | ì¥ì  | ë‹¨ì  | ì„ íƒ ì—¬ë¶€ |
|------|------|------|------|----------|
| **nomic-embed-text** | 768 | - ë¡œì»¬ ì‹¤í–‰<br>- ë¹ ë¦„<br>- ë¹„ìš© ì—†ìŒ | - OpenAIë³´ë‹¤ í’ˆì§ˆ ë‚®ìŒ | â­ **ì„ íƒ** (Phase 1) |
| **OpenAI text-embedding-3-small** | 1536 | - ë†’ì€ í’ˆì§ˆ<br>- ì•ˆì •ì  | - ë¹„ìš© ë°œìƒ<br>- API ì˜ì¡´ | ë³´ë¥˜ (Phase 2) |
| **OpenAI text-embedding-3-large** | 3072 | - ìµœê³  í’ˆì§ˆ | - ë¹„ìš© ë†’ìŒ<br>- ëŠë¦¼ | ë³´ë¥˜ |

### 3.2 ìµœì¢… ì„ íƒ: **nomic-embed-text** (Ollama)

**ì„ íƒ ì´ìœ **:
1. **ë¡œì»¬ ì‹¤í–‰**: ì™¸ë¶€ API ì˜ì¡´ì„± ì—†ìŒ
2. **ë¹„ìš© íš¨ìœ¨**: ë¬´ë£Œ
3. **ì„±ëŠ¥**: ë¡œì»¬ì—ì„œ ë¹ ë¥¸ ì‘ë‹µ (GPU ì‚¬ìš© ì‹œ)
4. **ê²€ì¦ í›„ ì „í™˜ ê°€ëŠ¥**: Task 2.5aì—ì„œ í’ˆì§ˆ í‰ê°€ í›„ OpenAI ì „í™˜ ê°€ëŠ¥

**nomic-embed-text ìŠ¤í™**:
- **ì°¨ì›**: 768
- **ìµœëŒ€ ì…ë ¥**: 8192 í† í°
- **ì†ë„**: ~50ms per chunk (GPU), ~200ms (CPU)
- **í’ˆì§ˆ**: Retrieval ì‘ì—…ì— ìµœì í™”

### 3.3 ë°°ì¹˜ ì²˜ë¦¬ ì „ëµ

**ì™œ ë°°ì¹˜ ì²˜ë¦¬?**
- **ì„±ëŠ¥**: 5ê°œ ë¬¸ì„œ ë³‘ë ¬ ì²˜ë¦¬ â†’ 5ë°° ë¹ ë¦„
- **ì•ˆì •ì„±**: 1ê°œ ì‹¤íŒ¨í•´ë„ ë‚˜ë¨¸ì§€ ê³„ì† ì§„í–‰
- **ë¦¬ì†ŒìŠ¤ íš¨ìœ¨**: Ollama ë™ì‹œ ìš”ì²­ ì²˜ë¦¬

**ë°°ì¹˜ í¬ê¸°: 5**
- **ê·¼ê±°**: Ollama ê¸°ë³¸ ë™ì‹œ ì²˜ë¦¬ ì œí•œ (~4-8)
- **ë©”ëª¨ë¦¬**: 5 chunks Ã— 768 dim Ã— 4 bytes â‰ˆ 15KB (ì¶©ë¶„íˆ ì‘ìŒ)
- **ì—ëŸ¬ìœ¨**: ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ê°€ëŠ¥í•œ í¬ê¸°

---

## 4. êµ¬í˜„ ë‹¨ê³„ë³„ ìƒì„¸ ê³„íš

### 4.1 Step 1: í™˜ê²½ ì„¤ì • ë° ì˜ì¡´ì„± ì„¤ì¹˜ (20ë¶„)

#### ì‘ì—… ë‚´ìš©
1. **requirements.txt ì—…ë°ì´íŠ¸**
   ```txt
   langchain-community==0.0.20
   ollama==0.1.6
   tenacity==8.2.3  # ì¬ì‹œë„ ë¡œì§ìš©
   ```

2. **ì˜ì¡´ì„± ì„¤ì¹˜**
   ```bash
   source venv/bin/activate
   pip install langchain-community==0.0.20 ollama==0.1.6 tenacity==8.2.3
   pip freeze > requirements.txt
   ```

3. **íŒŒì¼ ìƒì„±**
   ```bash
   touch app/services/document_indexer.py
   touch app/services/embedding_service.py
   touch tests/test_document_indexer.py
   touch tests/test_embedding_service.py
   ```

#### ê²€ì¦
```bash
# Ollama Python í´ë¼ì´ì–¸íŠ¸ í™•ì¸
python -c "import ollama; print(ollama.__version__)"

# tenacity í™•ì¸
python -c "from tenacity import retry; print('OK')"
```

---

### 4.2 Step 2: ì„ë² ë”© ì„œë¹„ìŠ¤ êµ¬í˜„ (60ë¶„)

#### ì‘ì—… ë‚´ìš©
`app/services/embedding_service.py` ì‘ì„±

**ì„¤ê³„ ì›ì¹™**:
- **ë‹¨ì¼ ì±…ì„**: ì„ë² ë”© ìƒì„±ë§Œ ë‹´ë‹¹
- **ì¬ì‹œë„ ë¡œì§**: ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ ëŒ€ì‘
- **íƒ€ì… ì•ˆì „ì„±**: ì…ì¶œë ¥ íƒ€ì… ëª…ì‹œ

```python
import logging
from typing import List, Optional
from pydantic import BaseModel, Field
import ollama
from tenacity import retry, stop_after_attempt, wait_exponential

logger = logging.getLogger(__name__)


class EmbeddingConfig(BaseModel):
    """ì„ë² ë”© ì„œë¹„ìŠ¤ ì„¤ì •"""

    model_name: str = Field(default="nomic-embed-text", description="ì„ë² ë”© ëª¨ë¸ëª…")
    expected_dimension: int = Field(default=768, description="ì˜ˆìƒ ì„ë² ë”© ì°¨ì›")
    batch_size: int = Field(default=5, ge=1, le=20, description="ë°°ì¹˜ í¬ê¸°")
    max_retries: int = Field(default=3, ge=1, le=10, description="ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜")


class EmbeddingServiceError(Exception):
    """ì„ë² ë”© ì„œë¹„ìŠ¤ ê¸°ë³¸ ì—ëŸ¬"""
    pass


class EmbeddingDimensionError(EmbeddingServiceError):
    """ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜ ì—ëŸ¬"""
    pass


class OllamaEmbeddingService:
    """Ollama ì„ë² ë”© ì„œë¹„ìŠ¤"""

    def __init__(self, config: Optional[EmbeddingConfig] = None):
        """
        Args:
            config: ì„ë² ë”© ì„¤ì •
        """
        self.config = config or EmbeddingConfig()
        self.client = ollama.Client()

        logger.info(
            f"OllamaEmbeddingService ì´ˆê¸°í™”: model={self.config.model_name}, "
            f"dimension={self.config.expected_dimension}"
        )

        # ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        self._verify_model_exists()

    def _verify_model_exists(self) -> None:
        """
        Ollama ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸

        Raises:
            EmbeddingServiceError: ëª¨ë¸ì´ ì—†ì„ ë•Œ
        """
        try:
            models = self.client.list()
            model_names = [model["name"] for model in models.get("models", [])]

            if self.config.model_name not in model_names:
                raise EmbeddingServiceError(
                    f"Ollama ëª¨ë¸ '{self.config.model_name}'ì´ ì—†ìŠµë‹ˆë‹¤. "
                    f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”: "
                    f"ollama pull {self.config.model_name}"
                )

            logger.info(f"Ollama ëª¨ë¸ '{self.config.model_name}' í™•ì¸ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
            raise EmbeddingServiceError(f"Ollama ì—°ê²° ì‹¤íŒ¨: {e}")

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        reraise=True
    )
    def embed_text(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            List[float]: ì„ë² ë”© ë²¡í„° (768ì°¨ì›)

        Raises:
            EmbeddingServiceError: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨
            EmbeddingDimensionError: ì°¨ì› ë¶ˆì¼ì¹˜
        """
        if not text.strip():
            logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ ì…ë ¥, 0 ë²¡í„° ë°˜í™˜")
            return [0.0] * self.config.expected_dimension

        try:
            response = self.client.embeddings(
                model=self.config.model_name,
                prompt=text
            )

            embedding = response["embedding"]

            # ì°¨ì› ê²€ì¦
            if len(embedding) != self.config.expected_dimension:
                raise EmbeddingDimensionError(
                    f"ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: {len(embedding)} "
                    f"(ì˜ˆìƒ: {self.config.expected_dimension})"
                )

            return embedding

        except EmbeddingDimensionError:
            raise
        except Exception as e:
            logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise EmbeddingServiceError(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")

    def embed_batch(self, texts: List[str]) -> List[List[float]]:
        """
        ë°°ì¹˜ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[List[float]]: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸

        Raises:
            EmbeddingServiceError: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨
        """
        if not texts:
            return []

        logger.info(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹œì‘: {len(texts)}ê°œ í…ìŠ¤íŠ¸")

        embeddings = []
        failed_indices = []

        for idx, text in enumerate(texts):
            try:
                embedding = self.embed_text(text)
                embeddings.append(embedding)
            except Exception as e:
                logger.error(f"í…ìŠ¤íŠ¸ {idx} ì„ë² ë”© ì‹¤íŒ¨: {e}")
                failed_indices.append(idx)
                # ì‹¤íŒ¨í•œ ê²½ìš° 0 ë²¡í„°ë¡œ ëŒ€ì²´
                embeddings.append([0.0] * self.config.expected_dimension)

        if failed_indices:
            logger.warning(
                f"ë°°ì¹˜ ì„ë² ë”© ì¤‘ {len(failed_indices)}ê°œ ì‹¤íŒ¨: {failed_indices}"
            )

        logger.info(f"ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ: {len(embeddings)}ê°œ ìƒì„±")

        return embeddings

    def get_embedding_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ë°˜í™˜"""
        return self.config.expected_dimension
```

#### í…ŒìŠ¤íŠ¸ ì‘ì„±
`tests/test_embedding_service.py`:

```python
import pytest
from app.services.embedding_service import (
    OllamaEmbeddingService,
    EmbeddingConfig,
    EmbeddingServiceError,
    EmbeddingDimensionError,
)


@pytest.fixture
def embedding_service():
    """ê¸°ë³¸ ì„ë² ë”© ì„œë¹„ìŠ¤"""
    return OllamaEmbeddingService()


def test_embed_single_text(embedding_service):
    """TC01: ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
    text = "This is a test sentence for embedding."

    embedding = embedding_service.embed_text(text)

    assert isinstance(embedding, list)
    assert len(embedding) == 768
    assert all(isinstance(x, float) for x in embedding)


def test_embed_korean_text(embedding_service):
    """TC02: í•œê¸€ í…ìŠ¤íŠ¸ ì„ë² ë”©"""
    text = "ì´ê²ƒì€ í•œê¸€ í…ìŠ¤íŠ¸ ì„ë² ë”© í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."

    embedding = embedding_service.embed_text(text)

    assert len(embedding) == 768


def test_embed_empty_text(embedding_service):
    """TC03: ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬"""
    text = ""

    embedding = embedding_service.embed_text(text)

    # 0 ë²¡í„° ë°˜í™˜
    assert len(embedding) == 768
    assert all(x == 0.0 for x in embedding)


def test_embed_batch(embedding_service):
    """TC04: ë°°ì¹˜ ì„ë² ë”©"""
    texts = [
        "First sentence.",
        "Second sentence.",
        "Third sentence.",
    ]

    embeddings = embedding_service.embed_batch(texts)

    assert len(embeddings) == 3
    assert all(len(emb) == 768 for emb in embeddings)


def test_embedding_dimension(embedding_service):
    """TC05: ì„ë² ë”© ì°¨ì› í™•ì¸"""
    dimension = embedding_service.get_embedding_dimension()

    assert dimension == 768
```

---

### 4.3 Step 3: Document Indexer êµ¬í˜„ (120min)

#### ì‘ì—… ë‚´ìš©
`app/services/document_indexer.py` ì‘ì„±

**ì„¤ê³„ ì›ì¹™**:
- **íŠ¸ëœì­ì…˜**: PostgreSQL + Milvus ì›ìì„±
- **ë¡¤ë°±**: ì‹¤íŒ¨ ì‹œ ì´ì „ ìƒíƒœ ë³µêµ¬
- **ë°°ì¹˜ ì²˜ë¦¬**: 5ê°œ ë¬¸ì„œ ë³‘ë ¬

```python
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pydantic import BaseModel, Field
from sqlalchemy.orm import Session
from pymilvus import Collection, utility

from app.services.document_parser.factory import DocumentParserFactory
from app.services.text_chunker import DocumentChunker
from app.services.embedding_service import OllamaEmbeddingService
from app.db.milvus_client import get_milvus_collection
from app.models.document import Document  # SQLAlchemy ëª¨ë¸
from app.db.session import get_db

logger = logging.getLogger(__name__)


class IndexingResult(BaseModel):
    """ì¸ë±ì‹± ê²°ê³¼"""

    success: bool = Field(..., description="ì„±ê³µ ì—¬ë¶€")
    document_id: Optional[int] = Field(None, description="ë¬¸ì„œ ID (ì„±ê³µ ì‹œ)")
    file_path: str = Field(..., description="íŒŒì¼ ê²½ë¡œ")
    total_chunks: int = Field(default=0, description="ìƒì„±ëœ ì²­í¬ ìˆ˜")
    indexed_chunks: int = Field(default=0, description="ì¸ë±ì‹±ëœ ì²­í¬ ìˆ˜")
    error_message: Optional[str] = Field(None, description="ì—ëŸ¬ ë©”ì‹œì§€ (ì‹¤íŒ¨ ì‹œ)")
    processing_time_ms: int = Field(..., description="ì²˜ë¦¬ ì‹œê°„ (ë°€ë¦¬ì´ˆ)")


class DocumentIndexerConfig(BaseModel):
    """ë¬¸ì„œ ì¸ë±ì„œ ì„¤ì •"""

    batch_size: int = Field(default=5, ge=1, le=20, description="ë°°ì¹˜ í¬ê¸°")
    max_retries: int = Field(default=3, ge=1, le=10, description="ìµœëŒ€ ì¬ì‹œë„")
    collection_name: str = Field(default="documents", description="Milvus Collectionëª…")


class DocumentIndexer:
    """ë¬¸ì„œ ì¸ë±ì„œ (íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥)"""

    def __init__(
        self,
        db_session: Session,
        config: Optional[DocumentIndexerConfig] = None
    ):
        """
        Args:
            db_session: SQLAlchemy ì„¸ì…˜
            config: ì¸ë±ì„œ ì„¤ì •
        """
        self.db = db_session
        self.config = config or DocumentIndexerConfig()

        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.chunker = DocumentChunker()
        self.embedding_service = OllamaEmbeddingService()

        # Milvus Collection
        self.collection = get_milvus_collection(self.config.collection_name)

        logger.info(
            f"DocumentIndexer ì´ˆê¸°í™”: batch_size={self.config.batch_size}"
        )

    def index_document(self, file_path: str) -> IndexingResult:
        """
        ë‹¨ì¼ ë¬¸ì„œ ì¸ë±ì‹± (ì „ì²´ íŒŒì´í”„ë¼ì¸)

        Args:
            file_path: ë¬¸ì„œ íŒŒì¼ ê²½ë¡œ

        Returns:
            IndexingResult: ì¸ë±ì‹± ê²°ê³¼
        """
        import time
        start_time = time.time()

        logger.info(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘: {file_path}")

        try:
            # Step 1: ë¬¸ì„œ íŒŒì‹±
            parser = DocumentParserFactory.create_parser(file_path)
            parsed_doc = parser.parse(file_path)

            logger.info(
                f"íŒŒì‹± ì™„ë£Œ: {parsed_doc.total_pages}í˜ì´ì§€, "
                f"{parsed_doc.total_characters}ì"
            )

            # Step 2: ì²­í‚¹
            chunks = self.chunker.chunk_document(parsed_doc)

            logger.info(f"ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")

            if not chunks:
                raise ValueError("ì²­í¬ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ (ë¹ˆ ë¬¸ì„œ)")

            # Step 3: PostgreSQLì— ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥
            document = self._save_document_metadata(file_path, parsed_doc)

            logger.info(f"ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: document_id={document.id}")

            # Step 4: ì„ë² ë”© ìƒì„±
            chunk_texts = [chunk.content for chunk in chunks]
            embeddings = self.embedding_service.embed_batch(chunk_texts)

            logger.info(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ")

            # Step 5: Milvusì— ì €ì¥
            indexed_count = self._save_to_milvus(
                document_id=document.id,
                chunks=chunks,
                embeddings=embeddings,
                parsed_doc=parsed_doc
            )

            logger.info(f"Milvus ì €ì¥ ì™„ë£Œ: {indexed_count}ê°œ ì²­í¬")

            # Step 6: ë¬¸ì„œ ìƒíƒœ ì—…ë°ì´íŠ¸
            document.indexed_at = datetime.utcnow()
            document.chunk_count = indexed_count
            self.db.commit()

            # ê²°ê³¼ ë°˜í™˜
            processing_time_ms = int((time.time() - start_time) * 1000)

            return IndexingResult(
                success=True,
                document_id=document.id,
                file_path=file_path,
                total_chunks=len(chunks),
                indexed_chunks=indexed_count,
                processing_time_ms=processing_time_ms
            )

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì¸ë±ì‹± ì‹¤íŒ¨: {e}", exc_info=True)
            self.db.rollback()

            processing_time_ms = int((time.time() - start_time) * 1000)

            return IndexingResult(
                success=False,
                file_path=file_path,
                error_message=str(e),
                processing_time_ms=processing_time_ms
            )

    def index_batch(self, file_paths: List[str]) -> List[IndexingResult]:
        """
        ë°°ì¹˜ ë¬¸ì„œ ì¸ë±ì‹±

        Args:
            file_paths: íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸

        Returns:
            List[IndexingResult]: ì¸ë±ì‹± ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"ë°°ì¹˜ ì¸ë±ì‹± ì‹œì‘: {len(file_paths)}ê°œ ë¬¸ì„œ")

        results = []

        # ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í• 
        for i in range(0, len(file_paths), self.config.batch_size):
            batch = file_paths[i:i + self.config.batch_size]

            logger.info(f"ë°°ì¹˜ {i // self.config.batch_size + 1} ì²˜ë¦¬ ì¤‘...")

            for file_path in batch:
                result = self.index_document(file_path)
                results.append(result)

        # í†µê³„
        success_count = sum(1 for r in results if r.success)
        fail_count = len(results) - success_count

        logger.info(
            f"ë°°ì¹˜ ì¸ë±ì‹± ì™„ë£Œ: ì„±ê³µ {success_count}, ì‹¤íŒ¨ {fail_count}"
        )

        return results

    def _save_document_metadata(
        self, file_path: str, parsed_doc: Any
    ) -> Document:
        """
        PostgreSQLì— ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì €ì¥

        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            parsed_doc: íŒŒì‹±ëœ ë¬¸ì„œ

        Returns:
            Document: SQLAlchemy ëª¨ë¸
        """
        import os
        from pathlib import Path

        document = Document(
            title=parsed_doc.metadata.get("title") or Path(file_path).stem,
            file_path=file_path,
            file_type=Path(file_path).suffix.lower(),
            file_size=os.path.getsize(file_path),
            total_pages=parsed_doc.total_pages,
            total_characters=parsed_doc.total_characters,
            metadata=parsed_doc.metadata,
            created_at=datetime.utcnow(),
        )

        self.db.add(document)
        self.db.flush()  # ID ìƒì„± (commit ì „)

        return document

    def _save_to_milvus(
        self,
        document_id: int,
        chunks: List[Any],
        embeddings: List[List[float]],
        parsed_doc: Any
    ) -> int:
        """
        Milvusì— ë²¡í„° + ë©”íƒ€ë°ì´í„° ì €ì¥

        Args:
            document_id: ë¬¸ì„œ ID
            chunks: TextChunk ë¦¬ìŠ¤íŠ¸
            embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
            parsed_doc: íŒŒì‹±ëœ ë¬¸ì„œ

        Returns:
            int: ì €ì¥ëœ ì²­í¬ ìˆ˜

        Raises:
            Exception: Milvus ì €ì¥ ì‹¤íŒ¨
        """
        # Milvus ì—”í‹°í‹° êµ¬ì„±
        entities = []

        for chunk, embedding in zip(chunks, embeddings):
            entity = {
                "document_id": document_id,
                "chunk_index": chunk.chunk_index,
                "content": chunk.content,
                "embedding": embedding,
                "page_number": chunk.metadata.get("page_number", 1),
                "metadata": {
                    "document_title": chunk.metadata.get("document_title", ""),
                    "chunk_length": chunk.metadata.get("chunk_length", 0),
                    "total_chunks": chunk.metadata.get("total_chunks", 0),
                }
            }
            entities.append(entity)

        # Milvusì— ì‚½ì…
        try:
            # Collectionì— ë§ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            insert_data = [
                [e["document_id"] for e in entities],
                [e["chunk_index"] for e in entities],
                [e["content"] for e in entities],
                [e["embedding"] for e in entities],
                [e["page_number"] for e in entities],
                [e["metadata"] for e in entities],
            ]

            self.collection.insert(insert_data)
            self.collection.flush()

            logger.info(f"Milvusì— {len(entities)}ê°œ ì—”í‹°í‹° ì €ì¥ ì™„ë£Œ")

            return len(entities)

        except Exception as e:
            logger.error(f"Milvus ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def delete_document(self, document_id: int) -> bool:
        """
        ë¬¸ì„œ ì‚­ì œ (PostgreSQL + Milvus)

        Args:
            document_id: ë¬¸ì„œ ID

        Returns:
            bool: ì„±ê³µ ì—¬ë¶€
        """
        try:
            # Step 1: Milvusì—ì„œ ì‚­ì œ
            expr = f"document_id == {document_id}"
            self.collection.delete(expr)
            self.collection.flush()

            logger.info(f"Milvusì—ì„œ document_id={document_id} ì‚­ì œ ì™„ë£Œ")

            # Step 2: PostgreSQLì—ì„œ ì‚­ì œ
            document = self.db.query(Document).filter(
                Document.id == document_id
            ).first()

            if document:
                self.db.delete(document)
                self.db.commit()

                logger.info(f"PostgreSQLì—ì„œ document_id={document_id} ì‚­ì œ ì™„ë£Œ")

            return True

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            self.db.rollback()
            return False
```

---

### 4.4 Step 4: í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„± (60min)

#### ì‘ì—… ë‚´ìš©
`tests/test_document_indexer.py`:

```python
import pytest
from pathlib import Path
from sqlalchemy.orm import Session
from app.services.document_indexer import DocumentIndexer, IndexingResult
from app.db.session import get_db
from app.models.document import Document


@pytest.fixture
def db_session():
    """í…ŒìŠ¤íŠ¸ DB ì„¸ì…˜"""
    db = next(get_db())
    yield db
    db.rollback()
    db.close()


@pytest.fixture
def indexer(db_session):
    """ë¬¸ì„œ ì¸ë±ì„œ"""
    return DocumentIndexer(db_session=db_session)


def test_index_pdf_document(indexer):
    """TC01: PDF ë¬¸ì„œ ì¸ë±ì‹±"""
    pdf_path = "tests/fixtures/pdf/sample_valid.pdf"

    if not Path(pdf_path).exists():
        pytest.skip("PDF íŒŒì¼ ì—†ìŒ")

    result = indexer.index_document(pdf_path)

    assert result.success is True
    assert result.document_id is not None
    assert result.total_chunks > 0
    assert result.indexed_chunks == result.total_chunks


def test_index_docx_document(indexer):
    """TC02: DOCX ë¬¸ì„œ ì¸ë±ì‹±"""
    docx_path = "tests/fixtures/docx/sample_valid.docx"

    if not Path(docx_path).exists():
        pytest.skip("DOCX íŒŒì¼ ì—†ìŒ")

    result = indexer.index_document(docx_path)

    assert result.success is True
    assert result.indexed_chunks > 0


def test_postgresql_metadata_saved(indexer, db_session):
    """TC03: PostgreSQL ë©”íƒ€ë°ì´í„° ì €ì¥ í™•ì¸"""
    pdf_path = "tests/fixtures/pdf/sample_valid.pdf"

    if not Path(pdf_path).exists():
        pytest.skip("PDF íŒŒì¼ ì—†ìŒ")

    result = indexer.index_document(pdf_path)

    # DBì—ì„œ í™•ì¸
    document = db_session.query(Document).filter(
        Document.id == result.document_id
    ).first()

    assert document is not None
    assert document.title is not None
    assert document.file_path == pdf_path
    assert document.chunk_count == result.indexed_chunks


def test_milvus_vectors_saved(indexer):
    """TC04: Milvus ë²¡í„° ì €ì¥ í™•ì¸"""
    pdf_path = "tests/fixtures/pdf/sample_valid.pdf"

    if not Path(pdf_path).exists():
        pytest.skip("PDF íŒŒì¼ ì—†ìŒ")

    result = indexer.index_document(pdf_path)

    # Milvusì—ì„œ í™•ì¸
    collection = indexer.collection
    collection.load()

    expr = f"document_id == {result.document_id}"
    search_result = collection.query(expr=expr, output_fields=["document_id"])

    assert len(search_result) == result.indexed_chunks


def test_embedding_dimension(indexer):
    """TC05: ì„ë² ë”© ì°¨ì› ê²€ì¦ (768)"""
    pdf_path = "tests/fixtures/pdf/sample_valid.pdf"

    if not Path(pdf_path).exists():
        pytest.skip("PDF íŒŒì¼ ì—†ìŒ")

    result = indexer.index_document(pdf_path)

    # Milvusì—ì„œ ë²¡í„° í™•ì¸
    collection = indexer.collection
    collection.load()

    expr = f"document_id == {result.document_id}"
    vectors = collection.query(
        expr=expr,
        output_fields=["embedding"],
        limit=1
    )

    if vectors:
        embedding = vectors[0]["embedding"]
        assert len(embedding) == 768


def test_batch_indexing(indexer):
    """TC06: ë°°ì¹˜ ì¸ë±ì‹±"""
    file_paths = [
        "tests/fixtures/pdf/sample_valid.pdf",
        "tests/fixtures/docx/sample_valid.docx",
        "tests/fixtures/txt/sample_valid.txt",
    ]

    # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°
    existing_files = [f for f in file_paths if Path(f).exists()]

    if not existing_files:
        pytest.skip("í…ŒìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ")

    results = indexer.index_batch(existing_files)

    assert len(results) == len(existing_files)
    assert all(r.success for r in results)


def test_delete_document(indexer, db_session):
    """TC07: ë¬¸ì„œ ì‚­ì œ"""
    pdf_path = "tests/fixtures/pdf/sample_valid.pdf"

    if not Path(pdf_path).exists():
        pytest.skip("PDF íŒŒì¼ ì—†ìŒ")

    # ì¸ë±ì‹±
    result = indexer.index_document(pdf_path)
    document_id = result.document_id

    # ì‚­ì œ
    success = indexer.delete_document(document_id)

    assert success is True

    # PostgreSQL í™•ì¸
    document = db_session.query(Document).filter(
        Document.id == document_id
    ).first()
    assert document is None

    # Milvus í™•ì¸
    collection = indexer.collection
    expr = f"document_id == {document_id}"
    search_result = collection.query(expr=expr)
    assert len(search_result) == 0
```

---

### 4.5 Step 5: í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„± (30min)

#### ì‘ì—… ë‚´ìš©
`scripts/test_full_pipeline.py`:

```python
#!/usr/bin/env python3
"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

Usage:
    python scripts/test_full_pipeline.py
"""

import sys
from pathlib import Path
from app.services.document_indexer import DocumentIndexer
from app.db.session import get_db


def test_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“„ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")

    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ëª©ë¡
    test_docs = [
        "tests/fixtures/pdf/sample_valid.pdf",
        "tests/fixtures/docx/sample_valid.docx",
        "tests/fixtures/txt/sample_valid.txt",
        "tests/fixtures/markdown/sample_valid.md",
    ]

    # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°
    existing_docs = [doc for doc in test_docs if Path(doc).exists()]

    if not existing_docs:
        print("âŒ í…ŒìŠ¤íŠ¸ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False

    print(f"ğŸ“‹ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ: {len(existing_docs)}ê°œ\n")

    # DB ì„¸ì…˜ ìƒì„±
    db = next(get_db())

    try:
        # ì¸ë±ì„œ ìƒì„±
        indexer = DocumentIndexer(db_session=db)

        # ë°°ì¹˜ ì¸ë±ì‹±
        results = indexer.index_batch(existing_docs)

        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š ì¸ë±ì‹± ê²°ê³¼:")
        print("=" * 60)

        for result in results:
            status = "âœ… ì„±ê³µ" if result.success else "âŒ ì‹¤íŒ¨"
            print(f"\n{status}: {Path(result.file_path).name}")
            print(f"  - ë¬¸ì„œ ID: {result.document_id}")
            print(f"  - ì´ ì²­í¬: {result.total_chunks}")
            print(f"  - ì¸ë±ì‹±ëœ ì²­í¬: {result.indexed_chunks}")
            print(f"  - ì²˜ë¦¬ ì‹œê°„: {result.processing_time_ms}ms")

            if not result.success:
                print(f"  - ì—ëŸ¬: {result.error_message}")

        # í†µê³„
        success_count = sum(1 for r in results if r.success)
        total_chunks = sum(r.indexed_chunks for r in results)

        print("\n" + "=" * 60)
        print(f"âœ… ì„±ê³µ: {success_count}/{len(results)}")
        print(f"ğŸ“¦ ì´ ì²­í¬: {total_chunks}ê°œ")
        print("=" * 60)

        return success_count == len(results)

    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        return False

    finally:
        db.close()


if __name__ == "__main__":
    success = test_pipeline()
    sys.exit(0 if success else 1)
```

---

### 4.6 Step 6: Attu UI ê²€ì¦ ë° ë¬¸ì„œí™” (30min)

#### ì‘ì—… ë‚´ìš©

1. **Attu UIì—ì„œ ë²¡í„° í™•ì¸**
   ```bash
   # Attu UI ì ‘ì†
   open http://localhost:8080

   # Collection ì„ íƒ: documents
   # Entities í™•ì¸:
   # - document_id
   # - chunk_index
   # - content
   # - embedding (768ì°¨ì›)
   # - page_number
   # - metadata
   ```

2. **PostgreSQL ë°ì´í„° í™•ì¸**
   ```bash
   psql -h localhost -U postgres -d rag_platform

   # ë¬¸ì„œ ëª©ë¡ í™•ì¸
   SELECT id, title, file_type, chunk_count, indexed_at FROM documents;

   # íŠ¹ì • ë¬¸ì„œ ìƒì„¸
   SELECT * FROM documents WHERE id = 1;
   ```

3. **ë¬¸ì„œí™”** (`docs/api/document_indexer_usage.md`):

```markdown
# Document Indexer Usage Guide

## ì „ì²´ íŒŒì´í”„ë¼ì¸
íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥

## ì‚¬ìš© ì˜ˆì œ

### ë‹¨ì¼ ë¬¸ì„œ ì¸ë±ì‹±
\`\`\`python
from app.services.document_indexer import DocumentIndexer
from app.db.session import get_db

db = next(get_db())
indexer = DocumentIndexer(db_session=db)

result = indexer.index_document("document.pdf")

if result.success:
    print(f"ì„±ê³µ! ë¬¸ì„œ ID: {result.document_id}")
    print(f"ì²­í¬ ìˆ˜: {result.indexed_chunks}")
else:
    print(f"ì‹¤íŒ¨: {result.error_message}")
\`\`\`

### ë°°ì¹˜ ì¸ë±ì‹±
\`\`\`python
file_paths = ["doc1.pdf", "doc2.docx", "doc3.txt"]
results = indexer.index_batch(file_paths)

success_count = sum(1 for r in results if r.success)
print(f"ì„±ê³µ: {success_count}/{len(results)}")
\`\`\`

### ë¬¸ì„œ ì‚­ì œ
\`\`\`python
success = indexer.delete_document(document_id=1)
\`\`\`
```

---

## 5. ê²€ì¦ ë° ìˆ˜ë™ í…ŒìŠ¤íŠ¸

### 5.1 ìë™í™” í…ŒìŠ¤íŠ¸ ê²€ì¦
```bash
# ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/test_embedding_service.py -v
pytest tests/test_document_indexer.py -v

# í†µí•© í…ŒìŠ¤íŠ¸
python scripts/test_full_pipeline.py
```

### 5.2 ìˆ˜ë™ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] 10ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹± ì„±ê³µ
- [ ] Attu UIì—ì„œ ë²¡í„° í™•ì¸ (768ì°¨ì›)
- [ ] PostgreSQLì—ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸
- [ ] ì„ë² ë”© ì°¨ì› 768 ê²€ì¦
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì •ìƒ ë™ì‘
- [ ] ì¬ì‹œë„ ë¡œì§ ë™ì‘ (ë„¤íŠ¸ì›Œí¬ ëŠì—ˆë‹¤ ì—°ê²°)
- [ ] ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦ (ì¤‘ê°„ì— ì—ëŸ¬ ë°œìƒ)

### 5.3 ì„±ëŠ¥ ê²€ì¦
```bash
# 10ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹œê°„ ì¸¡ì •
time python scripts/test_full_pipeline.py

# ëª©í‘œ: < 5ë¶„ (10ê°œ ë¬¸ì„œ)
```

---

## 6. ì‚°ì¶œë¬¼ (Deliverables)

### 6.1 ì½”ë“œ íŒŒì¼
- [x] `app/services/embedding_service.py` (ì„ë² ë”© ì„œë¹„ìŠ¤)
- [x] `app/services/document_indexer.py` (ë¬¸ì„œ ì¸ë±ì„œ)
- [x] `tests/test_embedding_service.py` (5ê°œ í…ŒìŠ¤íŠ¸)
- [x] `tests/test_document_indexer.py` (7ê°œ í…ŒìŠ¤íŠ¸)
- [x] `scripts/test_full_pipeline.py` (í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸)

### 6.2 ë¬¸ì„œ
- [x] `docs/api/document_indexer_usage.md` (ì‚¬ìš© ê°€ì´ë“œ)

---

## 7. ë¦¬ìŠ¤í¬ ë° ëŒ€ì‘ ë°©ì•ˆ

### 7.1 ê¸°ìˆ  ë¦¬ìŠ¤í¬

#### Risk 1: Ollama ì„ë² ë”© í’ˆì§ˆ ë¶€ì¡±
**ì¦ìƒ**:
- ê²€ìƒ‰ ì •í™•ë„ ë‚®ìŒ
- ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œ ê²€ìƒ‰ë¨

**í™•ë¥ **: Medium (30%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - Task 1.8 ì™„ë£Œ í›„ í’ˆì§ˆ í‰ê°€ (10ê°œ ìƒ˜í”Œ ê²€ìƒ‰)
   - ê²€ìƒ‰ ì •í™•ë„ ì¸¡ì •

2. **ë‹¨ê¸° ëŒ€ì‘** (Task 2.5a):
   - OpenAI embeddingìœ¼ë¡œ ì „í™˜ ì¤€ë¹„
   - ì„ë² ë”© ì„œë¹„ìŠ¤ ì¶”ìƒí™” (Provider íŒ¨í„´)

---

#### Risk 2: Milvus ì €ì¥ ì‹¤íŒ¨ (ë¡¤ë°± ë¯¸ì‘ë™)
**ì¦ìƒ**:
- PostgreSQLì—ëŠ” ì €ì¥, Milvusì—ëŠ” ì—†ìŒ
- ë°ì´í„° ë¶ˆì¼ì¹˜

**í™•ë¥ **: Low (15%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - íŠ¸ëœì­ì…˜ ê´€ë¦¬ ê°•í™”
   - Milvus ì €ì¥ ì‹¤íŒ¨ ì‹œ PostgreSQL ë¡¤ë°±

2. **ë‹¨ê¸° ëŒ€ì‘**:
   - ì •í•©ì„± ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
   - ì£¼ê¸°ì  ê²€ì¦ (Cron Job)

---

#### Risk 3: ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ë©”ëª¨ë¦¬ ë¶€ì¡±
**ì¦ìƒ**:
- 10,000+ ì²­í¬ ë¬¸ì„œ ì²˜ë¦¬ ì‹œ OOM
- ì„œë²„ ë‹¤ìš´

**í™•ë¥ **: Medium (20%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - ë°°ì¹˜ í¬ê¸° ì¡°ì • (5 â†’ 3)
   - ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¶”ê°€

2. **ë‹¨ê¸° ëŒ€ì‘**:
   - ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë³€ê²½
   - ì²­í¬ë³„ ìˆœì°¨ ì²˜ë¦¬

---

#### Risk 4: Ollama ì—°ê²° ì‹¤íŒ¨ (ì¬ì‹œë„ ê³ ê°ˆ)
**ì¦ìƒ**:
- 3íšŒ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨
- ì¸ë±ì‹± ì¤‘ë‹¨

**í™•ë¥ **: Low (10%)

**ëŒ€ì‘**:
1. **ì¦‰ì‹œ ëŒ€ì‘**:
   - ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€ (3 â†’ 5)
   - Exponential backoff ì ìš©

2. **ë‹¨ê¸° ëŒ€ì‘**:
   - Ollama Health check
   - ìë™ ì¬ì‹œì‘ ìŠ¤í¬ë¦½íŠ¸

---

## 8. Next Steps (Task 1.8 ì™„ë£Œ í›„)

### 8.1 ì¦‰ì‹œ ìˆ˜í–‰
1. **Phase 1 ì™„ë£Œ ê²€ì¦**
   - ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ (íŒŒì‹± â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥)
   - 10ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì„±ê³µ í™•ì¸
   - Attu UI + PostgreSQL ë°ì´í„° í™•ì¸

2. **ì½”ë“œ ë¦¬ë·° ìš”ì²­**
   - Backend Leadì—ê²Œ ë¦¬ë·° ìš”ì²­
   - ë³´ì•ˆ, ì„±ëŠ¥, ì•ˆì •ì„± ê²€í† 

3. **Git ì»¤ë°‹**
   ```bash
   git add .
   git commit -m "feat: Implement document indexing pipeline (Task 1.8)

   - Add OllamaEmbeddingService with nomic-embed-text
   - Add DocumentIndexer (parse â†’ chunk â†’ embed â†’ store)
   - Implement batch processing (5 documents parallel)
   - Add retry logic (3 attempts with exponential backoff)
   - Add rollback mechanism (PostgreSQL + Milvus)
   - Add comprehensive tests (embedding + indexer)
   - Add full pipeline integration test script
   - Verify 768-dimension embeddings in Milvus
   - Complete Phase 1: Document Processing Pipeline

   Closes #8"
   ```

### 8.2 Phase 1 ì™„ë£Œ ë¦¬ë·°
**Success Criteria í™•ì¸**:
- [ ] ë¬¸ì„œ ì¸ë±ì‹± ì„±ê³µ (10ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ)
- [ ] Milvusì— ë²¡í„° ì €ì¥ í™•ì¸ (Attu UI)
- [ ] PostgreSQLì— ë©”íƒ€ë°ì´í„° ì €ì¥ í™•ì¸
- [ ] Ollama ëª¨ë¸ ì •ìƒ ë™ì‘ (llama3, nomic-embed-text)

### 8.3 Phase 2 ì¤€ë¹„
**Task 2.1: FastAPI ê¸°ë³¸ êµ¬ì¡° ë° ë¼ìš°í„° ì„¤ì •** ì¤€ë¹„:
- [ ] FastAPI í”„ë¡œì íŠ¸ êµ¬ì¡° ê²€í† 
- [ ] API ì—”ë“œí¬ì¸íŠ¸ ì„¤ê³„
- [ ] ì¸ì¦/ì¸ê°€ ì „ëµ ê²€í† 

---

## 9. Appendix

### 9.1 ì°¸ê³  ìë£Œ
- [Ollama Python Library](https://github.com/ollama/ollama-python)
- [nomic-embed-text Model Card](https://huggingface.co/nomic-ai/nomic-embed-text-v1)
- [Milvus Python SDK](https://milvus.io/docs/install-pymilvus.md)
- [tenacity Retry Library](https://tenacity.readthedocs.io/)

### 9.2 Milvus Collection ìŠ¤í‚¤ë§ˆ (ì°¸ê³ )

```python
# Task 1.3ì—ì„œ ìƒì„±ë¨
schema = CollectionSchema(
    fields=[
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="document_id", dtype=DataType.INT64),
        FieldSchema(name="chunk_index", dtype=DataType.INT64),
        FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=2000),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=768),
        FieldSchema(name="page_number", dtype=DataType.INT64),
        FieldSchema(name="metadata", dtype=DataType.JSON),
    ],
    description="Documents collection"
)

# ì¸ë±ìŠ¤: HNSW (M=16, efConstruction=256), ë©”íŠ¸ë¦­: COSINE
```

### 9.3 ìœ ìš©í•œ ì»¤ë§¨ë“œ

```bash
# Ollama ëª¨ë¸ í™•ì¸
docker exec -it ollama ollama list

# nomic-embed-text í…ŒìŠ¤íŠ¸
docker exec -it ollama ollama run nomic-embed-text "test"

# Milvus ì—°ê²° í…ŒìŠ¤íŠ¸
python -c "from pymilvus import connections; connections.connect('default', host='localhost', port='19530'); print('OK')"

# Attu UI ì ‘ì†
open http://localhost:8080

# PostgreSQL ì ‘ì†
psql -h localhost -U postgres -d rag_platform

# ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
python scripts/test_full_pipeline.py
```

### 9.4 íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

**ë¬¸ì œ**: `ModuleNotFoundError: No module named 'ollama'`
**í•´ê²°**:
```bash
source venv/bin/activate
pip install ollama==0.1.6
```

**ë¬¸ì œ**: Ollama ì—°ê²° ì‹¤íŒ¨
**í•´ê²°**:
```bash
# Ollama ì»¨í…Œì´ë„ˆ ì¬ì‹œì‘
docker restart ollama

# ëª¨ë¸ ë‹¤ì‹œ ë‹¤ìš´ë¡œë“œ
docker exec -it ollama ollama pull nomic-embed-text
```

**ë¬¸ì œ**: Milvus ì €ì¥ ì‹¤íŒ¨ (ì°¨ì› ë¶ˆì¼ì¹˜)
**í•´ê²°**:
```python
# ì„ë² ë”© ì°¨ì› í™•ì¸
embedding = embedding_service.embed_text("test")
print(f"Dimension: {len(embedding)}")  # 768ì´ì–´ì•¼ í•¨

# Collection ìŠ¤í‚¤ë§ˆ í™•ì¸
collection.schema
```

**ë¬¸ì œ**: PostgreSQL ë¡¤ë°± ì•ˆ ë¨
**í•´ê²°**:
```python
# íŠ¸ëœì­ì…˜ ëª…ì‹œì  ê´€ë¦¬
try:
    # ... ì¸ë±ì‹± ì‘ì—… ...
    db.commit()
except Exception:
    db.rollback()
    raise
```

---

## 10. Approval & Sign-off

### 10.1 ì²´í¬ë¦¬ìŠ¤íŠ¸
Task 1.8 ì™„ë£Œ ì¡°ê±´:
- [ ] 10ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¸ë±ì‹± ì„±ê³µ
- [ ] Attu UIì—ì„œ ë²¡í„° í™•ì¸ (768ì°¨ì›)
- [ ] PostgreSQLì— ë©”íƒ€ë°ì´í„° ì €ì¥ í™•ì¸
- [ ] ì„ë² ë”© ì°¨ì› ê²€ì¦ (768ì°¨ì›)
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì •ìƒ ë™ì‘ (5ê°œ ë³‘ë ¬)
- [ ] ì¬ì‹œë„ ë¡œì§ ë™ì‘ í™•ì¸
- [ ] ë¡¤ë°± ë©”ì»¤ë‹ˆì¦˜ ê²€ì¦
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ë™ì‘
- [ ] ì„±ëŠ¥: 10ê°œ ë¬¸ì„œ < 5ë¶„
- [ ] ì½”ë“œ ì»¤ë²„ë¦¬ì§€ â‰¥ 85%
- [ ] ì½”ë“œ ë¦¬ë·° ìŠ¹ì¸
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

### 10.2 Phase 1 ì™„ë£Œ ìŠ¹ì¸
- [ ] **Backend Lead**: _______________
- [ ] **Tech Lead**: _______________
- [ ] **Infrastructure Team**: _______________

**Review Deadline**: Task 1.8 ì™„ë£Œ í›„ 48ì‹œê°„ ì´ë‚´

---

**END OF EXECUTION PLAN - PHASE 1 COMPLETE!** ğŸ‰
